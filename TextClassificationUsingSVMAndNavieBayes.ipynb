{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Naveen Kusakula - B00781205\n",
    "#### Theja Manasa Thatiparti - B00813459"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7Rcg5LaekTXx"
   },
   "source": [
    "### Q1. Collocation extraction . ( 35 marks )\n",
    "> a. Tokenize this corpus and perform part-of-speech tagging on it. ( 5 marks )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7Rcg5LaekTXx"
   },
   "source": [
    ">import dataset fetch_20newsgroups from Scikit learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IBUeQfCeaAqY"
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IUYijfxmaAqd"
   },
   "outputs": [],
   "source": [
    "import os, ssl\n",
    "if (not os.environ.get('PYTHONHTTPSVERIFY', '') and\n",
    "    getattr(ssl, '_create_unverified_context', None)): \n",
    "    ssl._create_default_https_context = ssl._create_unverified_context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qRLIg4fZi7EC"
   },
   "source": [
    ">* Area of interest for this assignment is below four newsgroups \n",
    ">* *Shuffle* and *random_state* are used to increase the randomness in the dataset which will be useful while training and splitting the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lpw51CmiaAqi"
   },
   "outputs": [],
   "source": [
    "interest = ['alt.atheism', 'talk.religion.misc','comp.graphics','sci.space',]\n",
    "newsgroups_train = fetch_20newsgroups(subset='train',categories=interest,shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7sE3qQw6koEq"
   },
   "source": [
    "> Displaying sample data element stored in array of arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 459
    },
    "colab_type": "code",
    "id": "3V8tlDCUaAqm",
    "outputId": "01567622-662b-42ef-b210-8c0f64eaa5b0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From: rych@festival.ed.ac.uk (R Hawkes)\n",
      "Subject: 3DS: Where did all the texture rules go?\n",
      "Lines: 21\n",
      "\n",
      "Hi,\n",
      "\n",
      "I've noticed that if you only save a model (with all your mapping planes\n",
      "positioned carefully) to a .3DS file that when you reload it after restarting\n",
      "3DS, they are given a default position and orientation.  But if you save\n",
      "to a .PRJ file their positions/orientation are preserved.  Does anyone\n",
      "know why this information is not stored in the .3DS file?  Nothing is\n",
      "explicitly said in the manual about saving texture rules in the .PRJ file. \n",
      "I'd like to be able to read the texture rule information, does anyone have \n",
      "the format for the .PRJ file?\n",
      "\n",
      "Is the .CEL file format available from somewhere?\n",
      "\n",
      "Rych\n",
      "\n",
      "======================================================================\n",
      "Rycharde Hawkes\t\t\t\temail: rych@festival.ed.ac.uk\n",
      "Virtual Environment Laboratory\n",
      "Dept. of Psychology\t\t\tTel  : +44 31 650 3426\n",
      "Univ. of Edinburgh\t\t\tFax  : +44 31 667 0150\n",
      "======================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(newsgroups_train.data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mB2VqwvkcWMK"
   },
   "source": [
    "#### Tokenization and Parts of speech tagging for uncleaned corpus\n",
    "\n",
    "> Ref: https://www.datacamp.com/community/tutorials/text-analytics-beginners-nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bw9aOSgFlqml"
   },
   "source": [
    "> * import nltk,pandas libraries\n",
    "* *word_tokenize*  tokenizes this dataset into array of arrays of word strings\n",
    "* As we have array of arrays we used for loop with iteration continued till the length of the dataset and appended each array of tokens to get  result containing a single array of tokens.\n",
    "* This process will be helpful **to calculate and compare the frequency of words in the entire corpus**, which is meaningful compared to the frequency results within only a part of dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "UIHX9TvuZDpo",
    "outputId": "b39e6005-4267-40b6-8066-76bbee03c99f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading collection 'all'\n",
      "[nltk_data]    | \n",
      "[nltk_data]    | Downloading package abc to\n",
      "[nltk_data]    |     /Users/naveenkusakula/nltk_data...\n",
      "[nltk_data]    |   Package abc is already up-to-date!\n",
      "[nltk_data]    | Downloading package alpino to\n",
      "[nltk_data]    |     /Users/naveenkusakula/nltk_data...\n",
      "[nltk_data]    |   Package alpino is already up-to-date!\n",
      "[nltk_data]    | Downloading package biocreative_ppi to\n",
      "[nltk_data]    |     /Users/naveenkusakula/nltk_data...\n",
      "[nltk_data]    |   Package biocreative_ppi is already up-to-date!\n",
      "[nltk_data]    | Downloading package brown to\n",
      "[nltk_data]    |     /Users/naveenkusakula/nltk_data...\n",
      "[nltk_data]    |   Package brown is already up-to-date!\n",
      "[nltk_data]    | Downloading package brown_tei to\n",
      "[nltk_data]    |     /Users/naveenkusakula/nltk_data...\n",
      "[nltk_data]    |   Package brown_tei is already up-to-date!\n",
      "[nltk_data]    | Downloading package cess_cat to\n",
      "[nltk_data]    |     /Users/naveenkusakula/nltk_data...\n",
      "[nltk_data]    |   Package cess_cat is already up-to-date!\n",
      "[nltk_data]    | Downloading package cess_esp to\n",
      "[nltk_data]    |     /Users/naveenkusakula/nltk_data...\n",
      "[nltk_data]    |   Package cess_esp is already up-to-date!\n",
      "[nltk_data]    | Downloading package chat80 to\n",
      "[nltk_data]    |     /Users/naveenkusakula/nltk_data...\n",
      "[nltk_data]    |   Package chat80 is already up-to-date!\n",
      "[nltk_data]    | Downloading package city_database to\n",
      "[nltk_data]    |     /Users/naveenkusakula/nltk_data...\n",
      "[nltk_data]    |   Package city_database is already up-to-date!\n",
      "[nltk_data]    | Downloading package cmudict to\n",
      "[nltk_data]    |     /Users/naveenkusakula/nltk_data...\n",
      "[nltk_data]    |   Package cmudict is already up-to-date!\n",
      "[nltk_data]    | Downloading package comparative_sentences to\n",
      "[nltk_data]    |     /Users/naveenkusakula/nltk_data...\n",
      "[nltk_data]    |   Package comparative_sentences is already up-to-\n",
      "[nltk_data]    |       date!\n",
      "[nltk_data]    | Downloading package comtrans to\n",
      "[nltk_data]    |     /Users/naveenkusakula/nltk_data...\n",
      "[nltk_data]    |   Package comtrans is already up-to-date!\n",
      "[nltk_data]    | Downloading package conll2000 to\n",
      "[nltk_data]    |     /Users/naveenkusakula/nltk_data...\n",
      "[nltk_data]    |   Package conll2000 is already up-to-date!\n",
      "[nltk_data]    | Downloading package conll2002 to\n",
      "[nltk_data]    |     /Users/naveenkusakula/nltk_data...\n",
      "[nltk_data]    |   Package conll2002 is already up-to-date!\n",
      "[nltk_data]    | Downloading package conll2007 to\n",
      "[nltk_data]    |     /Users/naveenkusakula/nltk_data...\n",
      "[nltk_data]    |   Package conll2007 is already up-to-date!\n",
      "[nltk_data]    | Downloading package crubadan to\n",
      "[nltk_data]    |     /Users/naveenkusakula/nltk_data...\n",
      "[nltk_data]    |   Package crubadan is already up-to-date!\n",
      "[nltk_data]    | Downloading package dependency_treebank to\n",
      "[nltk_data]    |     /Users/naveenkusakula/nltk_data...\n",
      "[nltk_data]    |   Package dependency_treebank is already up-to-date!\n",
      "[nltk_data]    | Downloading package dolch to\n",
      "[nltk_data]    |     /Users/naveenkusakula/nltk_data...\n",
      "[nltk_data]    |   Package dolch is already up-to-date!\n",
      "[nltk_data]    | Downloading package europarl_raw to\n",
      "[nltk_data]    |     /Users/naveenkusakula/nltk_data...\n",
      "[nltk_data]    |   Package europarl_raw is already up-to-date!\n",
      "[nltk_data]    | Downloading package floresta to\n",
      "[nltk_data]    |     /Users/naveenkusakula/nltk_data...\n",
      "[nltk_data]    |   Package floresta is already up-to-date!\n",
      "[nltk_data]    | Downloading package framenet_v15 to\n",
      "[nltk_data]    |     /Users/naveenkusakula/nltk_data...\n",
      "[nltk_data]    |   Package framenet_v15 is already up-to-date!\n",
      "[nltk_data]    | Downloading package framenet_v17 to\n",
      "[nltk_data]    |     /Users/naveenkusakula/nltk_data...\n",
      "[nltk_data]    |   Package framenet_v17 is already up-to-date!\n",
      "[nltk_data]    | Downloading package gazetteers to\n",
      "[nltk_data]    |     /Users/naveenkusakula/nltk_data...\n",
      "[nltk_data]    |   Package gazetteers is already up-to-date!\n",
      "[nltk_data]    | Downloading package genesis to\n",
      "[nltk_data]    |     /Users/naveenkusakula/nltk_data...\n",
      "[nltk_data]    |   Package genesis is already up-to-date!\n",
      "[nltk_data]    | Downloading package gutenberg to\n",
      "[nltk_data]    |     /Users/naveenkusakula/nltk_data...\n",
      "[nltk_data]    |   Package gutenberg is already up-to-date!\n",
      "[nltk_data]    | Downloading package ieer to\n",
      "[nltk_data]    |     /Users/naveenkusakula/nltk_data...\n",
      "[nltk_data]    |   Package ieer is already up-to-date!\n",
      "[nltk_data]    | Downloading package inaugural to\n",
      "[nltk_data]    |     /Users/naveenkusakula/nltk_data...\n",
      "[nltk_data]    |   Package inaugural is already up-to-date!\n",
      "[nltk_data]    | Downloading package indian to\n",
      "[nltk_data]    |     /Users/naveenkusakula/nltk_data...\n",
      "[nltk_data]    |   Package indian is already up-to-date!\n",
      "[nltk_data]    | Downloading package jeita to\n",
      "[nltk_data]    |     /Users/naveenkusakula/nltk_data...\n",
      "[nltk_data]    |   Package jeita is already up-to-date!\n",
      "[nltk_data]    | Downloading package kimmo to\n",
      "[nltk_data]    |     /Users/naveenkusakula/nltk_data...\n",
      "[nltk_data]    |   Package kimmo is already up-to-date!\n",
      "[nltk_data]    | Downloading package knbc to\n",
      "[nltk_data]    |     /Users/naveenkusakula/nltk_data...\n",
      "[nltk_data]    |   Package knbc is already up-to-date!\n",
      "[nltk_data]    | Downloading package lin_thesaurus to\n",
      "[nltk_data]    |     /Users/naveenkusakula/nltk_data...\n",
      "[nltk_data]    |   Package lin_thesaurus is already up-to-date!\n",
      "[nltk_data]    | Downloading package mac_morpho to\n",
      "[nltk_data]    |     /Users/naveenkusakula/nltk_data...\n",
      "[nltk_data]    |   Package mac_morpho is already up-to-date!\n",
      "[nltk_data]    | Downloading package machado to\n",
      "[nltk_data]    |     /Users/naveenkusakula/nltk_data...\n",
      "[nltk_data]    |   Package machado is already up-to-date!\n",
      "[nltk_data]    | Downloading package masc_tagged to\n",
      "[nltk_data]    |     /Users/naveenkusakula/nltk_data...\n",
      "[nltk_data]    |   Package masc_tagged is already up-to-date!\n",
      "[nltk_data]    | Downloading package moses_sample to\n",
      "[nltk_data]    |     /Users/naveenkusakula/nltk_data...\n",
      "[nltk_data]    |   Package moses_sample is already up-to-date!\n",
      "[nltk_data]    | Downloading package movie_reviews to\n",
      "[nltk_data]    |     /Users/naveenkusakula/nltk_data...\n",
      "[nltk_data]    |   Package movie_reviews is already up-to-date!\n",
      "[nltk_data]    | Downloading package names to\n",
      "[nltk_data]    |     /Users/naveenkusakula/nltk_data...\n",
      "[nltk_data]    |   Package names is already up-to-date!\n",
      "[nltk_data]    | Downloading package nombank.1.0 to\n",
      "[nltk_data]    |     /Users/naveenkusakula/nltk_data...\n",
      "[nltk_data]    |   Package nombank.1.0 is already up-to-date!\n",
      "[nltk_data]    | Downloading package nps_chat to\n",
      "[nltk_data]    |     /Users/naveenkusakula/nltk_data...\n",
      "[nltk_data]    |   Package nps_chat is already up-to-date!\n",
      "[nltk_data]    | Downloading package omw to\n",
      "[nltk_data]    |     /Users/naveenkusakula/nltk_data...\n",
      "[nltk_data]    |   Package omw is already up-to-date!\n",
      "[nltk_data]    | Downloading package opinion_lexicon to\n",
      "[nltk_data]    |     /Users/naveenkusakula/nltk_data...\n",
      "[nltk_data]    |   Package opinion_lexicon is already up-to-date!\n",
      "[nltk_data]    | Downloading package paradigms to\n",
      "[nltk_data]    |     /Users/naveenkusakula/nltk_data...\n",
      "[nltk_data]    |   Package paradigms is already up-to-date!\n",
      "[nltk_data]    | Downloading package pil to\n",
      "[nltk_data]    |     /Users/naveenkusakula/nltk_data...\n",
      "[nltk_data]    |   Package pil is already up-to-date!\n",
      "[nltk_data]    | Downloading package pl196x to\n",
      "[nltk_data]    |     /Users/naveenkusakula/nltk_data...\n",
      "[nltk_data]    |   Package pl196x is already up-to-date!\n",
      "[nltk_data]    | Downloading package ppattach to\n",
      "[nltk_data]    |     /Users/naveenkusakula/nltk_data...\n",
      "[nltk_data]    |   Package ppattach is already up-to-date!\n",
      "[nltk_data]    | Downloading package problem_reports to\n",
      "[nltk_data]    |     /Users/naveenkusakula/nltk_data...\n",
      "[nltk_data]    |   Package problem_reports is already up-to-date!\n",
      "[nltk_data]    | Downloading package propbank to\n",
      "[nltk_data]    |     /Users/naveenkusakula/nltk_data...\n",
      "[nltk_data]    |   Package propbank is already up-to-date!\n",
      "[nltk_data]    | Downloading package ptb to\n",
      "[nltk_data]    |     /Users/naveenkusakula/nltk_data...\n",
      "[nltk_data]    |   Package ptb is already up-to-date!\n",
      "[nltk_data]    | Downloading package product_reviews_1 to\n",
      "[nltk_data]    |     /Users/naveenkusakula/nltk_data...\n",
      "[nltk_data]    |   Package product_reviews_1 is already up-to-date!\n",
      "[nltk_data]    | Downloading package product_reviews_2 to\n",
      "[nltk_data]    |     /Users/naveenkusakula/nltk_data...\n",
      "[nltk_data]    |   Package product_reviews_2 is already up-to-date!\n",
      "[nltk_data]    | Downloading package pros_cons to\n",
      "[nltk_data]    |     /Users/naveenkusakula/nltk_data...\n",
      "[nltk_data]    |   Package pros_cons is already up-to-date!\n",
      "[nltk_data]    | Downloading package qc to\n",
      "[nltk_data]    |     /Users/naveenkusakula/nltk_data...\n",
      "[nltk_data]    |   Package qc is already up-to-date!\n",
      "[nltk_data]    | Downloading package reuters to\n",
      "[nltk_data]    |     /Users/naveenkusakula/nltk_data...\n",
      "[nltk_data]    |   Package reuters is already up-to-date!\n",
      "[nltk_data]    | Downloading package rte to\n",
      "[nltk_data]    |     /Users/naveenkusakula/nltk_data...\n",
      "[nltk_data]    |   Package rte is already up-to-date!\n",
      "[nltk_data]    | Downloading package semcor to\n",
      "[nltk_data]    |     /Users/naveenkusakula/nltk_data...\n",
      "[nltk_data]    |   Package semcor is already up-to-date!\n",
      "[nltk_data]    | Downloading package senseval to\n",
      "[nltk_data]    |     /Users/naveenkusakula/nltk_data...\n",
      "[nltk_data]    |   Package senseval is already up-to-date!\n",
      "[nltk_data]    | Downloading package sentiwordnet to\n",
      "[nltk_data]    |     /Users/naveenkusakula/nltk_data...\n",
      "[nltk_data]    |   Package sentiwordnet is already up-to-date!\n",
      "[nltk_data]    | Downloading package sentence_polarity to\n",
      "[nltk_data]    |     /Users/naveenkusakula/nltk_data...\n",
      "[nltk_data]    |   Package sentence_polarity is already up-to-date!\n",
      "[nltk_data]    | Downloading package shakespeare to\n",
      "[nltk_data]    |     /Users/naveenkusakula/nltk_data...\n",
      "[nltk_data]    |   Package shakespeare is already up-to-date!\n",
      "[nltk_data]    | Downloading package sinica_treebank to\n",
      "[nltk_data]    |     /Users/naveenkusakula/nltk_data...\n",
      "[nltk_data]    |   Package sinica_treebank is already up-to-date!\n",
      "[nltk_data]    | Downloading package smultron to\n",
      "[nltk_data]    |     /Users/naveenkusakula/nltk_data...\n",
      "[nltk_data]    |   Package smultron is already up-to-date!\n",
      "[nltk_data]    | Downloading package state_union to\n",
      "[nltk_data]    |     /Users/naveenkusakula/nltk_data...\n",
      "[nltk_data]    |   Package state_union is already up-to-date!\n",
      "[nltk_data]    | Downloading package stopwords to\n",
      "[nltk_data]    |     /Users/naveenkusakula/nltk_data...\n",
      "[nltk_data]    |   Package stopwords is already up-to-date!\n",
      "[nltk_data]    | Downloading package subjectivity to\n",
      "[nltk_data]    |     /Users/naveenkusakula/nltk_data...\n",
      "[nltk_data]    |   Package subjectivity is already up-to-date!\n",
      "[nltk_data]    | Downloading package swadesh to\n",
      "[nltk_data]    |     /Users/naveenkusakula/nltk_data...\n",
      "[nltk_data]    |   Package swadesh is already up-to-date!\n",
      "[nltk_data]    | Downloading package switchboard to\n",
      "[nltk_data]    |     /Users/naveenkusakula/nltk_data...\n",
      "[nltk_data]    |   Package switchboard is already up-to-date!\n",
      "[nltk_data]    | Downloading package timit to\n",
      "[nltk_data]    |     /Users/naveenkusakula/nltk_data...\n",
      "[nltk_data]    |   Package timit is already up-to-date!\n",
      "[nltk_data]    | Downloading package toolbox to\n",
      "[nltk_data]    |     /Users/naveenkusakula/nltk_data...\n",
      "[nltk_data]    |   Package toolbox is already up-to-date!\n",
      "[nltk_data]    | Downloading package treebank to\n",
      "[nltk_data]    |     /Users/naveenkusakula/nltk_data...\n",
      "[nltk_data]    |   Package treebank is already up-to-date!\n",
      "[nltk_data]    | Downloading package twitter_samples to\n",
      "[nltk_data]    |     /Users/naveenkusakula/nltk_data...\n",
      "[nltk_data]    |   Package twitter_samples is already up-to-date!\n",
      "[nltk_data]    | Downloading package udhr to\n",
      "[nltk_data]    |     /Users/naveenkusakula/nltk_data...\n",
      "[nltk_data]    |   Package udhr is already up-to-date!\n",
      "[nltk_data]    | Downloading package udhr2 to\n",
      "[nltk_data]    |     /Users/naveenkusakula/nltk_data...\n",
      "[nltk_data]    |   Package udhr2 is already up-to-date!\n",
      "[nltk_data]    | Downloading package unicode_samples to\n",
      "[nltk_data]    |     /Users/naveenkusakula/nltk_data...\n",
      "[nltk_data]    |   Package unicode_samples is already up-to-date!\n",
      "[nltk_data]    | Downloading package universal_treebanks_v20 to\n",
      "[nltk_data]    |     /Users/naveenkusakula/nltk_data...\n",
      "[nltk_data]    |   Package universal_treebanks_v20 is already up-to-\n",
      "[nltk_data]    |       date!\n",
      "[nltk_data]    | Downloading package verbnet to\n",
      "[nltk_data]    |     /Users/naveenkusakula/nltk_data...\n",
      "[nltk_data]    |   Package verbnet is already up-to-date!\n",
      "[nltk_data]    | Downloading package verbnet3 to\n",
      "[nltk_data]    |     /Users/naveenkusakula/nltk_data...\n",
      "[nltk_data]    |   Package verbnet3 is already up-to-date!\n",
      "[nltk_data]    | Downloading package webtext to\n",
      "[nltk_data]    |     /Users/naveenkusakula/nltk_data...\n",
      "[nltk_data]    |   Package webtext is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet to\n",
      "[nltk_data]    |     /Users/naveenkusakula/nltk_data...\n",
      "[nltk_data]    |   Package wordnet is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet_ic to\n",
      "[nltk_data]    |     /Users/naveenkusakula/nltk_data...\n",
      "[nltk_data]    |   Package wordnet_ic is already up-to-date!\n",
      "[nltk_data]    | Downloading package words to\n",
      "[nltk_data]    |     /Users/naveenkusakula/nltk_data...\n",
      "[nltk_data]    |   Package words is already up-to-date!\n",
      "[nltk_data]    | Downloading package ycoe to\n",
      "[nltk_data]    |     /Users/naveenkusakula/nltk_data...\n",
      "[nltk_data]    |   Package ycoe is already up-to-date!\n",
      "[nltk_data]    | Downloading package rslp to\n",
      "[nltk_data]    |     /Users/naveenkusakula/nltk_data...\n",
      "[nltk_data]    |   Package rslp is already up-to-date!\n",
      "[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n",
      "[nltk_data]    |     /Users/naveenkusakula/nltk_data...\n",
      "[nltk_data]    |   Package maxent_treebank_pos_tagger is already up-\n",
      "[nltk_data]    |       to-date!\n",
      "[nltk_data]    | Downloading package universal_tagset to\n",
      "[nltk_data]    |     /Users/naveenkusakula/nltk_data...\n",
      "[nltk_data]    |   Package universal_tagset is already up-to-date!\n",
      "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
      "[nltk_data]    |     /Users/naveenkusakula/nltk_data...\n",
      "[nltk_data]    |   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data]    | Downloading package punkt to\n",
      "[nltk_data]    |     /Users/naveenkusakula/nltk_data...\n",
      "[nltk_data]    |   Package punkt is already up-to-date!\n",
      "[nltk_data]    | Downloading package book_grammars to\n",
      "[nltk_data]    |     /Users/naveenkusakula/nltk_data...\n",
      "[nltk_data]    |   Package book_grammars is already up-to-date!\n",
      "[nltk_data]    | Downloading package sample_grammars to\n",
      "[nltk_data]    |     /Users/naveenkusakula/nltk_data...\n",
      "[nltk_data]    |   Package sample_grammars is already up-to-date!\n",
      "[nltk_data]    | Downloading package spanish_grammars to\n",
      "[nltk_data]    |     /Users/naveenkusakula/nltk_data...\n",
      "[nltk_data]    |   Package spanish_grammars is already up-to-date!\n",
      "[nltk_data]    | Downloading package basque_grammars to\n",
      "[nltk_data]    |     /Users/naveenkusakula/nltk_data...\n",
      "[nltk_data]    |   Package basque_grammars is already up-to-date!\n",
      "[nltk_data]    | Downloading package large_grammars to\n",
      "[nltk_data]    |     /Users/naveenkusakula/nltk_data...\n",
      "[nltk_data]    |   Package large_grammars is already up-to-date!\n",
      "[nltk_data]    | Downloading package tagsets to\n",
      "[nltk_data]    |     /Users/naveenkusakula/nltk_data...\n",
      "[nltk_data]    |   Package tagsets is already up-to-date!\n",
      "[nltk_data]    | Downloading package snowball_data to\n",
      "[nltk_data]    |     /Users/naveenkusakula/nltk_data...\n",
      "[nltk_data]    |   Package snowball_data is already up-to-date!\n",
      "[nltk_data]    | Downloading package bllip_wsj_no_aux to\n",
      "[nltk_data]    |     /Users/naveenkusakula/nltk_data...\n",
      "[nltk_data]    |   Package bllip_wsj_no_aux is already up-to-date!\n",
      "[nltk_data]    | Downloading package word2vec_sample to\n",
      "[nltk_data]    |     /Users/naveenkusakula/nltk_data...\n",
      "[nltk_data]    |   Package word2vec_sample is already up-to-date!\n",
      "[nltk_data]    | Downloading package panlex_swadesh to\n",
      "[nltk_data]    |     /Users/naveenkusakula/nltk_data...\n",
      "[nltk_data]    |   Package panlex_swadesh is already up-to-date!\n",
      "[nltk_data]    | Downloading package mte_teip5 to\n",
      "[nltk_data]    |     /Users/naveenkusakula/nltk_data...\n",
      "[nltk_data]    |   Package mte_teip5 is already up-to-date!\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]    |     /Users/naveenkusakula/nltk_data...\n",
      "[nltk_data]    |   Package averaged_perceptron_tagger is already up-\n",
      "[nltk_data]    |       to-date!\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger_ru to\n",
      "[nltk_data]    |     /Users/naveenkusakula/nltk_data...\n",
      "[nltk_data]    |   Package averaged_perceptron_tagger_ru is already\n",
      "[nltk_data]    |       up-to-date!\n",
      "[nltk_data]    | Downloading package perluniprops to\n",
      "[nltk_data]    |     /Users/naveenkusakula/nltk_data...\n",
      "[nltk_data]    |   Package perluniprops is already up-to-date!\n",
      "[nltk_data]    | Downloading package nonbreaking_prefixes to\n",
      "[nltk_data]    |     /Users/naveenkusakula/nltk_data...\n",
      "[nltk_data]    |   Package nonbreaking_prefixes is already up-to-date!\n",
      "[nltk_data]    | Downloading package vader_lexicon to\n",
      "[nltk_data]    |     /Users/naveenkusakula/nltk_data...\n",
      "[nltk_data]    |   Package vader_lexicon is already up-to-date!\n",
      "[nltk_data]    | Downloading package porter_test to\n",
      "[nltk_data]    |     /Users/naveenkusakula/nltk_data...\n",
      "[nltk_data]    |   Package porter_test is already up-to-date!\n",
      "[nltk_data]    | Downloading package wmt15_eval to\n",
      "[nltk_data]    |     /Users/naveenkusakula/nltk_data...\n",
      "[nltk_data]    |   Package wmt15_eval is already up-to-date!\n",
      "[nltk_data]    | Downloading package mwa_ppdb to\n",
      "[nltk_data]    |     /Users/naveenkusakula/nltk_data...\n",
      "[nltk_data]    |   Package mwa_ppdb is already up-to-date!\n",
      "[nltk_data]    | \n",
      "[nltk_data]  Done downloading collection all\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('all')\n",
    "import pandas as pd\n",
    "from nltk import pos_tag\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "leq90Y4BaAqv"
   },
   "outputs": [],
   "source": [
    "#Tokenizing the words\n",
    "words=[]\n",
    "for i in range(len(newsgroups_train.data)):\n",
    "        document = newsgroups_train.data[i]\n",
    "        words += word_tokenize(document)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "TeyVUiQOaAqz",
    "outputId": "b18c5b03-8e5b-4dc5-dbce-b5d33d3f7430"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words after Tokenization:\n",
      "788086\n",
      "Sample words after Tokenization:\n",
      "['From', ':', 'rych', '@', 'festival.ed.ac.uk', '(', 'R', 'Hawkes', ')', 'Subject', ':', '3DS', ':', 'Where', 'did', 'all', 'the', 'texture', 'rules', 'go', '?', 'Lines', ':', '21', 'Hi', ',', 'I', \"'ve\", 'noticed', 'that']\n",
      "Sample words after POS tagging:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('From', 'IN'),\n",
       " (':', ':'),\n",
       " ('rych', 'NN'),\n",
       " ('@', 'NN'),\n",
       " ('festival.ed.ac.uk', 'NN'),\n",
       " ('(', '('),\n",
       " ('R', 'NNP'),\n",
       " ('Hawkes', 'NNP'),\n",
       " (')', ')'),\n",
       " ('Subject', 'NN'),\n",
       " (':', ':'),\n",
       " ('3DS', 'CD'),\n",
       " (':', ':'),\n",
       " ('Where', 'WRB'),\n",
       " ('did', 'VBD'),\n",
       " ('all', 'PDT'),\n",
       " ('the', 'DT'),\n",
       " ('texture', 'NN'),\n",
       " ('rules', 'NNS'),\n",
       " ('go', 'VB'),\n",
       " ('?', '.'),\n",
       " ('Lines', 'NNS'),\n",
       " (':', ':'),\n",
       " ('21', 'CD'),\n",
       " ('Hi', 'NNP'),\n",
       " (',', ','),\n",
       " ('I', 'PRP'),\n",
       " (\"'ve\", 'VBP'),\n",
       " ('noticed', 'VBN'),\n",
       " ('that', 'IN'),\n",
       " ('if', 'IN'),\n",
       " ('you', 'PRP'),\n",
       " ('only', 'RB'),\n",
       " ('save', 'VB'),\n",
       " ('a', 'DT'),\n",
       " ('model', 'NN'),\n",
       " ('(', '('),\n",
       " ('with', 'IN'),\n",
       " ('all', 'DT'),\n",
       " ('your', 'PRP$'),\n",
       " ('mapping', 'NN'),\n",
       " ('planes', 'NNS'),\n",
       " ('positioned', 'VBN'),\n",
       " ('carefully', 'RB'),\n",
       " (')', ')'),\n",
       " ('to', 'TO'),\n",
       " ('a', 'DT'),\n",
       " ('.3DS', 'NN'),\n",
       " ('file', 'NN'),\n",
       " ('that', 'IN'),\n",
       " ('when', 'WRB'),\n",
       " ('you', 'PRP'),\n",
       " ('reload', 'VBP'),\n",
       " ('it', 'PRP'),\n",
       " ('after', 'IN'),\n",
       " ('restarting', 'VBG'),\n",
       " ('3DS', 'CD'),\n",
       " (',', ','),\n",
       " ('they', 'PRP'),\n",
       " ('are', 'VBP'),\n",
       " ('given', 'VBN'),\n",
       " ('a', 'DT'),\n",
       " ('default', 'NN'),\n",
       " ('position', 'NN'),\n",
       " ('and', 'CC'),\n",
       " ('orientation', 'NN'),\n",
       " ('.', '.'),\n",
       " ('But', 'CC'),\n",
       " ('if', 'IN'),\n",
       " ('you', 'PRP'),\n",
       " ('save', 'VBP'),\n",
       " ('to', 'TO'),\n",
       " ('a', 'DT'),\n",
       " ('.PRJ', 'NN'),\n",
       " ('file', 'NN'),\n",
       " ('their', 'PRP$'),\n",
       " ('positions/orientation', 'NN'),\n",
       " ('are', 'VBP'),\n",
       " ('preserved', 'VBN'),\n",
       " ('.', '.'),\n",
       " ('Does', 'NNP'),\n",
       " ('anyone', 'NN'),\n",
       " ('know', 'VB'),\n",
       " ('why', 'WRB'),\n",
       " ('this', 'DT'),\n",
       " ('information', 'NN'),\n",
       " ('is', 'VBZ'),\n",
       " ('not', 'RB'),\n",
       " ('stored', 'VBN'),\n",
       " ('in', 'IN'),\n",
       " ('the', 'DT'),\n",
       " ('.3DS', 'NN'),\n",
       " ('file', 'NN'),\n",
       " ('?', '.'),\n",
       " ('Nothing', 'NN'),\n",
       " ('is', 'VBZ'),\n",
       " ('explicitly', 'RB'),\n",
       " ('said', 'VBD'),\n",
       " ('in', 'IN'),\n",
       " ('the', 'DT'),\n",
       " ('manual', 'JJ'),\n",
       " ('about', 'IN'),\n",
       " ('saving', 'VBG'),\n",
       " ('texture', 'NN'),\n",
       " ('rules', 'NNS'),\n",
       " ('in', 'IN'),\n",
       " ('the', 'DT'),\n",
       " ('.PRJ', 'NNP'),\n",
       " ('file', 'NN'),\n",
       " ('.', '.'),\n",
       " ('I', 'PRP'),\n",
       " (\"'d\", 'MD'),\n",
       " ('like', 'VB'),\n",
       " ('to', 'TO'),\n",
       " ('be', 'VB'),\n",
       " ('able', 'JJ'),\n",
       " ('to', 'TO'),\n",
       " ('read', 'VB'),\n",
       " ('the', 'DT'),\n",
       " ('texture', 'NN'),\n",
       " ('rule', 'NN'),\n",
       " ('information', 'NN'),\n",
       " (',', ','),\n",
       " ('does', 'VBZ'),\n",
       " ('anyone', 'NN'),\n",
       " ('have', 'VB'),\n",
       " ('the', 'DT'),\n",
       " ('format', 'NN'),\n",
       " ('for', 'IN'),\n",
       " ('the', 'DT'),\n",
       " ('.PRJ', 'NNP'),\n",
       " ('file', 'NN'),\n",
       " ('?', '.'),\n",
       " ('Is', 'VBZ'),\n",
       " ('the', 'DT'),\n",
       " ('.CEL', 'JJ'),\n",
       " ('file', 'NN'),\n",
       " ('format', 'NN'),\n",
       " ('available', 'JJ'),\n",
       " ('from', 'IN'),\n",
       " ('somewhere', 'RB'),\n",
       " ('?', '.'),\n",
       " ('Rych', 'NNP'),\n",
       " ('======================================================================',\n",
       "  'NNP'),\n",
       " ('Rycharde', 'NNP'),\n",
       " ('Hawkes', 'NNP'),\n",
       " ('email', 'NN'),\n",
       " (':', ':'),\n",
       " ('rych', 'NN'),\n",
       " ('@', 'NNP'),\n",
       " ('festival.ed.ac.uk', 'VBP'),\n",
       " ('Virtual', 'NNP'),\n",
       " ('Environment', 'NNP'),\n",
       " ('Laboratory', 'NNP'),\n",
       " ('Dept', 'NNP'),\n",
       " ('.', '.'),\n",
       " ('of', 'IN'),\n",
       " ('Psychology', 'NNP'),\n",
       " ('Tel', 'NNP'),\n",
       " (':', ':'),\n",
       " ('+44', 'NN'),\n",
       " ('31', 'CD'),\n",
       " ('650', 'CD'),\n",
       " ('3426', 'CD'),\n",
       " ('Univ', 'NNP'),\n",
       " ('.', '.'),\n",
       " ('of', 'IN'),\n",
       " ('Edinburgh', 'NNP'),\n",
       " ('Fax', 'NNP'),\n",
       " (':', ':'),\n",
       " ('+44', 'NN'),\n",
       " ('31', 'CD'),\n",
       " ('667', 'CD'),\n",
       " ('0150', 'CD'),\n",
       " ('======================================================================',\n",
       "  'NN'),\n",
       " ('Subject', 'NN'),\n",
       " (':', ':'),\n",
       " ('Re', 'NN'),\n",
       " (':', ':'),\n",
       " ('Biblical', 'JJ'),\n",
       " ('Backing', 'NNP'),\n",
       " ('of', 'IN'),\n",
       " ('Koresh', 'NNP'),\n",
       " (\"'s\", 'POS'),\n",
       " ('3-02', 'JJ'),\n",
       " ('Tape', 'NN'),\n",
       " ('(', '('),\n",
       " ('Cites', 'NNP'),\n",
       " ('enclosed', 'VBD'),\n",
       " (')', ')'),\n",
       " ('From', 'IN'),\n",
       " (':', ':'),\n",
       " ('kmcvay', 'NN'),\n",
       " ('@', 'NN'),\n",
       " ('oneb.almanac.bc.ca', 'NN'),\n",
       " ('(', '('),\n",
       " ('Ken', 'NNP'),\n",
       " ('Mcvay', 'NNP'),\n",
       " (')', ')'),\n",
       " ('Organization', 'NN'),\n",
       " (':', ':'),\n",
       " ('The', 'DT'),\n",
       " ('Old', 'NNP'),\n",
       " ('Frog', 'NNP'),\n",
       " (\"'s\", 'POS'),\n",
       " ('Almanac', 'NNP'),\n",
       " ('Lines', 'NNPS'),\n",
       " (':', ':'),\n",
       " ('20', 'CD'),\n",
       " ('In', 'IN'),\n",
       " ('article', 'NN'),\n",
       " ('<', '$'),\n",
       " ('20APR199301460499', 'CD'),\n",
       " ('@', 'NNP'),\n",
       " ('utarlg.uta.edu', 'JJ'),\n",
       " ('>', 'NNP'),\n",
       " ('b645zaw', 'NN'),\n",
       " ('@', 'NNP'),\n",
       " ('utarlg.uta.edu', 'NN'),\n",
       " ('(', '('),\n",
       " ('stephen', 'NN'),\n",
       " (')', ')'),\n",
       " ('writes', 'VBZ'),\n",
       " (':', ':'),\n",
       " ('>', 'NN'),\n",
       " ('Seems', 'VBZ'),\n",
       " ('to', 'TO'),\n",
       " ('me', 'PRP'),\n",
       " ('Koresh', 'NNP'),\n",
       " ('is', 'VBZ'),\n",
       " ('yet', 'RB'),\n",
       " ('another', 'DT'),\n",
       " ('messenger', 'NN'),\n",
       " ('that', 'WDT'),\n",
       " ('got', 'VBD'),\n",
       " ('killed', 'VBN'),\n",
       " ('>', 'NN'),\n",
       " ('for', 'IN'),\n",
       " ('the', 'DT'),\n",
       " ('message', 'NN'),\n",
       " ('he', 'PRP'),\n",
       " ('carried', 'VBD'),\n",
       " ('.', '.'),\n",
       " ('(', '('),\n",
       " ('Which', 'NNP'),\n",
       " ('says', 'VBZ'),\n",
       " ('nothing', 'NN'),\n",
       " ('about', 'IN'),\n",
       " ('the', 'DT'),\n",
       " ('Seems', 'NNPS'),\n",
       " ('to', 'TO'),\n",
       " ('be', 'VB'),\n",
       " (',', ','),\n",
       " ('barring', 'VBG'),\n",
       " ('evidence', 'NN'),\n",
       " ('to', 'TO'),\n",
       " ('the', 'DT'),\n",
       " ('contrary', 'JJ'),\n",
       " (',', ','),\n",
       " ('that', 'IN'),\n",
       " ('Koresh', 'NNP'),\n",
       " ('was', 'VBD'),\n",
       " ('simply', 'RB'),\n",
       " ('another', 'DT'),\n",
       " ('deranged', 'VBN'),\n",
       " ('fanatic', 'NN'),\n",
       " ('who', 'WP'),\n",
       " ('thought', 'VBD'),\n",
       " ('it', 'PRP'),\n",
       " ('neccessary', 'JJ'),\n",
       " ('to', 'TO'),\n",
       " ('take', 'VB'),\n",
       " ('a', 'DT'),\n",
       " ('whole', 'JJ'),\n",
       " ('bunch', 'NN'),\n",
       " ('of', 'IN'),\n",
       " ('folks', 'NNS'),\n",
       " ('with', 'IN'),\n",
       " ('him', 'PRP'),\n",
       " (',', ','),\n",
       " ('children', 'NNS'),\n",
       " ('and', 'CC'),\n",
       " ('all', 'DT'),\n",
       " (',', ','),\n",
       " ('to', 'TO'),\n",
       " ('satisfy', 'VB'),\n",
       " ('his', 'PRP$'),\n",
       " ('delusional', 'JJ'),\n",
       " ('mania', 'NN'),\n",
       " ('.', '.'),\n",
       " ('Jim', 'NNP'),\n",
       " ('Jones', 'NNP'),\n",
       " (',', ','),\n",
       " ('circa', 'NN'),\n",
       " ('1993', 'CD'),\n",
       " ('.', '.'),\n",
       " ('>', 'VB'),\n",
       " ('In', 'IN'),\n",
       " ('the', 'DT'),\n",
       " ('mean', 'JJ'),\n",
       " ('time', 'NN'),\n",
       " (',', ','),\n",
       " ('we', 'PRP'),\n",
       " ('sure', 'VBP'),\n",
       " ('learned', 'VBN'),\n",
       " ('a', 'DT'),\n",
       " ('lot', 'NN'),\n",
       " ('about', 'IN'),\n",
       " ('evil', 'NN'),\n",
       " ('and', 'CC'),\n",
       " ('corruption', 'NN'),\n",
       " ('.', '.'),\n",
       " ('>', 'CC'),\n",
       " ('Are', 'NNP'),\n",
       " ('you', 'PRP'),\n",
       " ('surprised', 'JJ'),\n",
       " ('things', 'NNS'),\n",
       " ('have', 'VBP'),\n",
       " ('gotten', 'VBN'),\n",
       " ('that', 'IN'),\n",
       " ('rotten', 'VB'),\n",
       " ('?', '.'),\n",
       " ('Nope', 'NNP'),\n",
       " ('-', ':'),\n",
       " ('fruitcakes', 'NNS'),\n",
       " ('like', 'IN'),\n",
       " ('Koresh', 'NNP'),\n",
       " ('have', 'VBP'),\n",
       " ('been', 'VBN'),\n",
       " ('demonstrating', 'VBG'),\n",
       " ('such', 'JJ'),\n",
       " ('evil', 'JJ'),\n",
       " ('corruption', 'NN'),\n",
       " ('for', 'IN'),\n",
       " ('centuries', 'NNS'),\n",
       " ('.', '.'),\n",
       " ('--', ':'),\n",
       " ('The', 'DT'),\n",
       " ('Old', 'NNP'),\n",
       " ('Frog', 'NNP'),\n",
       " (\"'s\", 'POS'),\n",
       " ('Almanac', 'NNP'),\n",
       " ('-', ':'),\n",
       " ('A', 'DT'),\n",
       " ('Salute', 'NN'),\n",
       " ('to', 'TO'),\n",
       " ('That', 'DT'),\n",
       " ('Old', 'NNP'),\n",
       " ('Frog', 'NNP'),\n",
       " ('Hisse', 'NNP'),\n",
       " (\"'\", 'POS'),\n",
       " ('f', 'NN'),\n",
       " (',', ','),\n",
       " ('Ryugen', 'NNP'),\n",
       " ('Fisher', 'NNP'),\n",
       " ('(', '('),\n",
       " ('604', 'CD'),\n",
       " (')', ')'),\n",
       " ('245-3205', 'CD'),\n",
       " ('(', '('),\n",
       " ('v32', 'NN'),\n",
       " (')', ')'),\n",
       " ('(', '('),\n",
       " ('604', 'CD'),\n",
       " (')', ')'),\n",
       " ('245-4366', 'CD'),\n",
       " ('(', '('),\n",
       " ('2400x4', 'CD'),\n",
       " (')', ')'),\n",
       " ('SCO', 'NNP'),\n",
       " ('XENIX', '$'),\n",
       " ('2.3.2', 'CD'),\n",
       " ('GT', 'NNP'),\n",
       " ('Ladysmith', 'NNP'),\n",
       " (',', ','),\n",
       " ('British', 'NNP'),\n",
       " ('Columbia', 'NNP'),\n",
       " (',', ','),\n",
       " ('CANADA', 'NNP'),\n",
       " ('.', '.'),\n",
       " ('Serving', 'VBG'),\n",
       " ('Central', 'JJ'),\n",
       " ('Vancouver', 'NNP'),\n",
       " ('Island', 'NNP'),\n",
       " ('with', 'IN'),\n",
       " ('public', 'JJ'),\n",
       " ('access', 'NN'),\n",
       " ('UseNet', 'NNP'),\n",
       " ('and', 'CC'),\n",
       " ('Internet', 'NNP'),\n",
       " ('Mail', 'NNP'),\n",
       " ('-', ':'),\n",
       " ('home', 'NN'),\n",
       " ('to', 'TO'),\n",
       " ('the', 'DT'),\n",
       " ('Holocaust', 'NNP'),\n",
       " ('Almanac', 'NNP'),\n",
       " ('From', 'IN'),\n",
       " (':', ':'),\n",
       " ('Mark.Perew', 'NNP'),\n",
       " ('@', 'NNP'),\n",
       " ('p201.f208.n103.z1.fidonet.org', 'NN'),\n",
       " ('Subject', 'NN'),\n",
       " (':', ':'),\n",
       " ('Re', 'NN'),\n",
       " (':', ':'),\n",
       " ('Comet', 'NNP'),\n",
       " ('in', 'IN'),\n",
       " ('Temporary', 'NNP'),\n",
       " ('Orbit', 'NNP'),\n",
       " ('Around', 'IN'),\n",
       " ('Jupiter', 'NNP'),\n",
       " ('?', '.'),\n",
       " ('X-Sender', 'NN'),\n",
       " (':', ':'),\n",
       " ('newtout', 'RB'),\n",
       " ('0.08', 'CD'),\n",
       " ('Feb', 'NNP'),\n",
       " ('23', 'CD'),\n",
       " ('1993', 'CD'),\n",
       " ('Lines', 'NNS'),\n",
       " (':', ':'),\n",
       " ('15', 'CD'),\n",
       " ('In', 'IN'),\n",
       " ('a', 'DT'),\n",
       " ('message', 'NN'),\n",
       " ('of', 'IN'),\n",
       " ('<', 'NNP'),\n",
       " ('Apr', 'NNP'),\n",
       " ('19', 'CD'),\n",
       " ('04:55', 'CD'),\n",
       " ('>', 'NN'),\n",
       " (',', ','),\n",
       " ('jgarland', 'NN'),\n",
       " ('@', 'NNP'),\n",
       " ('kean.ucs.mun.ca', 'NN'),\n",
       " ('writes', 'NNS'),\n",
       " (':', ':'),\n",
       " ('>', 'NN'),\n",
       " ('In', 'IN'),\n",
       " ('article', 'NN'),\n",
       " ('<', '$'),\n",
       " ('1993Apr19.020359.26996', 'CD'),\n",
       " ('@', 'NNP'),\n",
       " ('sq.sq.com', 'NN'),\n",
       " ('>', 'NNP'),\n",
       " (',', ','),\n",
       " ('msb', 'NN'),\n",
       " ('@', 'NNP'),\n",
       " ('sq.sq.com', 'NN'),\n",
       " ('(', '('),\n",
       " ('Mark', 'NNP'),\n",
       " ('Brader', 'NNP'),\n",
       " (')', ')'),\n",
       " ('>', 'NN'),\n",
       " ('writes', 'VBZ'),\n",
       " (':', ':'),\n",
       " ('MB', 'NNP'),\n",
       " ('>', 'NNP'),\n",
       " ('So', 'IN'),\n",
       " ('the', 'DT'),\n",
       " ('MB', 'NNP'),\n",
       " ('>', 'NN'),\n",
       " ('1970', 'CD'),\n",
       " ('figure', 'NN'),\n",
       " ('seems', 'VBZ'),\n",
       " ('unlikely', 'JJ'),\n",
       " ('to', 'TO'),\n",
       " ('actually', 'RB'),\n",
       " ('be', 'VB'),\n",
       " ('anything', 'NN'),\n",
       " ('but', 'CC'),\n",
       " ('a', 'DT'),\n",
       " ('perijove', 'NN'),\n",
       " ('.', '.'),\n",
       " ('JG', 'NNP'),\n",
       " ('>', 'NNP'),\n",
       " ('Sorry', 'NNP'),\n",
       " (',', ','),\n",
       " ('_perijoves_', 'NNP'),\n",
       " ('...', ':'),\n",
       " ('I', 'PRP'),\n",
       " (\"'m\", 'VBP'),\n",
       " ('not', 'RB'),\n",
       " ('used', 'VBN'),\n",
       " ('to', 'TO'),\n",
       " ('talking', 'VBG'),\n",
       " ('this', 'DT'),\n",
       " ('language', 'NN'),\n",
       " ('.', '.'),\n",
       " ('Could', 'MD'),\n",
       " (\"n't\", 'RB'),\n",
       " ('we', 'PRP'),\n",
       " ('just', 'RB'),\n",
       " ('say', 'VBP'),\n",
       " ('periapsis', 'NN'),\n",
       " ('or', 'CC'),\n",
       " ('apoapsis', 'NN'),\n",
       " ('?', '.'),\n",
       " ('--', ':'),\n",
       " ('-', ':'),\n",
       " ('msged', 'VBD'),\n",
       " ('2.07', 'CD'),\n",
       " ('From', 'IN'),\n",
       " (':', ':'),\n",
       " ('dpw', 'NN'),\n",
       " ('@', 'NN'),\n",
       " ('sei.cmu.edu', 'NN'),\n",
       " ('(', '('),\n",
       " ('David', 'NNP'),\n",
       " ('Wood', 'NNP'),\n",
       " (')', ')'),\n",
       " ('Subject', 'NN'),\n",
       " (':', ':'),\n",
       " ('Request', 'NN'),\n",
       " ('for', 'IN'),\n",
       " ('Support', 'NNP'),\n",
       " ('Organization', 'NNP'),\n",
       " (':', ':'),\n",
       " ('Software', 'NNP'),\n",
       " ('Engineering', 'NNP'),\n",
       " ('Institute', 'NNP'),\n",
       " ('Lines', 'NNPS'),\n",
       " (':', ':'),\n",
       " ('35', 'CD'),\n",
       " ('I', 'PRP'),\n",
       " ('have', 'VBP'),\n",
       " ('a', 'DT'),\n",
       " ('request', 'NN'),\n",
       " ('for', 'IN'),\n",
       " ('those', 'DT'),\n",
       " ('who', 'WP'),\n",
       " ('would', 'MD'),\n",
       " ('like', 'VB'),\n",
       " ('to', 'TO'),\n",
       " ('see', 'VB'),\n",
       " ('Charley', 'NNP'),\n",
       " ('Wingate', 'NNP'),\n",
       " ('respond', 'NN'),\n",
       " ('to', 'TO'),\n",
       " ('the', 'DT'),\n",
       " ('``', '``'),\n",
       " ('Charley', 'NNP'),\n",
       " ('Challenges', 'NNP'),\n",
       " (\"''\", \"''\"),\n",
       " ('(', '('),\n",
       " ('and', 'CC'),\n",
       " ('judging', 'VBG'),\n",
       " ('from', 'IN'),\n",
       " ('my', 'PRP$'),\n",
       " ('e-mail', 'NN'),\n",
       " (',', ','),\n",
       " ('there', 'EX'),\n",
       " ('appear', 'VB'),\n",
       " ('to', 'TO'),\n",
       " ('be', 'VB'),\n",
       " ('quite', 'RB'),\n",
       " ('a', 'DT'),\n",
       " ('few', 'JJ'),\n",
       " ('of', 'IN'),\n",
       " ('you', 'PRP'),\n",
       " ('.', '.'),\n",
       " (')', ')'),\n",
       " ('It', 'PRP'),\n",
       " ('is', 'VBZ'),\n",
       " ('clear', 'JJ'),\n",
       " ('that', 'IN'),\n",
       " ('Mr.', 'NNP'),\n",
       " ('Wingate', 'NNP'),\n",
       " ('intends', 'VBZ'),\n",
       " ('to', 'TO'),\n",
       " ('continue', 'VB'),\n",
       " ('to', 'TO'),\n",
       " ('post', 'VB'),\n",
       " ('tangential', 'JJ'),\n",
       " ('or', 'CC'),\n",
       " ('unrelated', 'JJ'),\n",
       " ('articles', 'NNS'),\n",
       " ('while', 'IN'),\n",
       " ('ingoring', 'VBG'),\n",
       " ('the', 'DT'),\n",
       " ('Challenges', 'NNP'),\n",
       " ('themselves', 'PRP'),\n",
       " ('.', '.'),\n",
       " ('Between', 'IN'),\n",
       " ('the', 'DT'),\n",
       " ('last', 'JJ'),\n",
       " ('two', 'CD'),\n",
       " ('re-postings', 'NNS'),\n",
       " ('of', 'IN'),\n",
       " ('the', 'DT'),\n",
       " ('Challenges', 'NNP'),\n",
       " (',', ','),\n",
       " ('I', 'PRP'),\n",
       " ('noted', 'VBD'),\n",
       " ('perhaps', 'RB'),\n",
       " ('a', 'DT'),\n",
       " ('dozen', 'NN'),\n",
       " ('or', 'CC'),\n",
       " ('more', 'JJR'),\n",
       " ('posts', 'NNS'),\n",
       " ('by', 'IN'),\n",
       " ('Mr.', 'NNP'),\n",
       " ('Wingate', 'NNP'),\n",
       " (',', ','),\n",
       " ('none', 'NN'),\n",
       " ('of', 'IN'),\n",
       " ('which', 'WDT'),\n",
       " ('answered', 'VBD'),\n",
       " ('a', 'DT'),\n",
       " ('single', 'JJ'),\n",
       " ('Challenge', 'NNP'),\n",
       " ('.', '.'),\n",
       " ('It', 'PRP'),\n",
       " ('seems', 'VBZ'),\n",
       " ('unmistakable', 'JJ'),\n",
       " ('to', 'TO'),\n",
       " ('me', 'PRP'),\n",
       " ('that', 'IN'),\n",
       " ('Mr.', 'NNP'),\n",
       " ('Wingate', 'NNP'),\n",
       " ('hopes', 'VBZ'),\n",
       " ('that', 'IN'),\n",
       " ('the', 'DT'),\n",
       " ('questions', 'NNS'),\n",
       " ('will', 'MD'),\n",
       " ('just', 'RB'),\n",
       " ('go', 'VB'),\n",
       " ('away', 'RB'),\n",
       " (',', ','),\n",
       " ('and', 'CC'),\n",
       " ('he', 'PRP'),\n",
       " ('is', 'VBZ'),\n",
       " ('doing', 'VBG'),\n",
       " ('his', 'PRP$'),\n",
       " ('level', 'NN'),\n",
       " ('best', 'JJS'),\n",
       " ('to', 'TO'),\n",
       " ('change', 'VB'),\n",
       " ('the', 'DT'),\n",
       " ('subject', 'NN'),\n",
       " ('.', '.'),\n",
       " ('Given', 'VBN'),\n",
       " ('that', 'IN'),\n",
       " ('this', 'DT'),\n",
       " ('seems', 'VBZ'),\n",
       " ('a', 'DT'),\n",
       " ('rather', 'RB'),\n",
       " ('common', 'JJ'),\n",
       " ('net.theist', 'JJ'),\n",
       " ('tactic', 'JJ'),\n",
       " (',', ','),\n",
       " ('I', 'PRP'),\n",
       " ('would', 'MD'),\n",
       " ('like', 'VB'),\n",
       " ('to', 'TO'),\n",
       " ('suggest', 'VB'),\n",
       " ('that', 'IN'),\n",
       " ('we', 'PRP'),\n",
       " ('impress', 'VBP'),\n",
       " ('upon', 'IN'),\n",
       " ('him', 'PRP'),\n",
       " ('our', 'PRP$'),\n",
       " ('desire', 'NN'),\n",
       " ('for', 'IN'),\n",
       " ('answers', 'NNS'),\n",
       " (',', ','),\n",
       " ('in', 'IN'),\n",
       " ('the', 'DT'),\n",
       " ('following', 'JJ'),\n",
       " ('manner', 'NN'),\n",
       " (':', ':'),\n",
       " ('1', 'CD'),\n",
       " ('.', '.'),\n",
       " ('Ignore', 'VB'),\n",
       " ('any', 'DT'),\n",
       " ('future', 'JJ'),\n",
       " ('articles', 'NNS'),\n",
       " ('by', 'IN'),\n",
       " ('Mr.', 'NNP'),\n",
       " ('Wingate', 'NNP'),\n",
       " ('that', 'WDT'),\n",
       " ('do', 'VBP'),\n",
       " ('not', 'RB'),\n",
       " ('address', 'VB'),\n",
       " ('the', 'DT'),\n",
       " ('Challenges', 'NNP'),\n",
       " (',', ','),\n",
       " ('until', 'IN'),\n",
       " ('he', 'PRP'),\n",
       " ('answers', 'VBZ'),\n",
       " ('them', 'PRP'),\n",
       " ('or', 'CC'),\n",
       " ('explictly', 'RB'),\n",
       " ('announces', 'NNS'),\n",
       " ('that', 'IN'),\n",
       " ('he', 'PRP'),\n",
       " ('refuses', 'VBZ'),\n",
       " ('to', 'TO'),\n",
       " ('do', 'VB'),\n",
       " ('so', 'RB'),\n",
       " ('.', '.'),\n",
       " ('--', ':'),\n",
       " ('or', 'CC'),\n",
       " ('--', ':'),\n",
       " ('2', 'CD'),\n",
       " ('.', '.'),\n",
       " ('If', 'IN'),\n",
       " ('you', 'PRP'),\n",
       " ('must', 'MD'),\n",
       " ('respond', 'VB'),\n",
       " ('to', 'TO'),\n",
       " ('one', 'CD'),\n",
       " ('of', 'IN'),\n",
       " ('his', 'PRP$'),\n",
       " ('articles', 'NNS'),\n",
       " (',', ','),\n",
       " ('include', 'VBP'),\n",
       " ('within', 'IN'),\n",
       " ('it', 'PRP'),\n",
       " ('something', 'NN'),\n",
       " ('similar', 'JJ'),\n",
       " ('to', 'TO'),\n",
       " ('the', 'DT'),\n",
       " ('following', 'NN'),\n",
       " (':', ':'),\n",
       " ('``', '``'),\n",
       " ('Please', 'NNP'),\n",
       " ('answer', 'VB'),\n",
       " ('the', 'DT'),\n",
       " ('questions', 'NNS'),\n",
       " ('posed', 'VBD'),\n",
       " ('to', 'TO'),\n",
       " ('you', 'PRP'),\n",
       " ('in', 'IN'),\n",
       " ('the', 'DT'),\n",
       " ('Charley', 'NNP'),\n",
       " ('Challenges', 'NNP'),\n",
       " ('.', '.'),\n",
       " (\"''\", \"''\"),\n",
       " ('Really', 'RB'),\n",
       " (',', ','),\n",
       " ('I', 'PRP'),\n",
       " (\"'m\", 'VBP'),\n",
       " ('not', 'RB'),\n",
       " ('looking', 'VBG'),\n",
       " ('to', 'TO'),\n",
       " ('humiliate', 'VB'),\n",
       " ('anyone', 'NN'),\n",
       " ('here', 'RB'),\n",
       " (',', ','),\n",
       " ('I', 'PRP'),\n",
       " ('just', 'RB'),\n",
       " ('want', 'VBP'),\n",
       " ('some', 'DT'),\n",
       " ('honest', 'JJS'),\n",
       " ('answers', 'NNS'),\n",
       " ('.', '.'),\n",
       " ('You', 'PRP'),\n",
       " ('would', 'MD'),\n",
       " (\"n't\", 'RB'),\n",
       " ('think', 'VB'),\n",
       " ('that', 'IN'),\n",
       " ('honesty', 'NN'),\n",
       " ('would', 'MD'),\n",
       " ('be', 'VB'),\n",
       " ('too', 'RB'),\n",
       " ('much', 'JJ'),\n",
       " ('to', 'TO'),\n",
       " ('ask', 'VB'),\n",
       " ('from', 'IN'),\n",
       " ('a', 'DT'),\n",
       " ('devout', 'NN'),\n",
       " ('Christian', 'NNP'),\n",
       " (',', ','),\n",
       " ('would', 'MD'),\n",
       " ('you', 'PRP'),\n",
       " ('?', '.'),\n",
       " ('Nevermind', 'UH'),\n",
       " (',', ','),\n",
       " ('that', 'DT'),\n",
       " ('was', 'VBD'),\n",
       " ('a', 'DT'),\n",
       " ('rhetorical', 'JJ'),\n",
       " ('question', 'NN'),\n",
       " ('.', '.'),\n",
       " ('--', ':'),\n",
       " ('Dave', 'VBP'),\n",
       " ('Wood', 'NN'),\n",
       " ('From', 'IN'),\n",
       " (':', ':'),\n",
       " ('prb', 'NN'),\n",
       " ('@', 'NN'),\n",
       " ('access.digex.com', 'NN'),\n",
       " ('(', '('),\n",
       " ('Pat', 'NNP'),\n",
       " (')', ')'),\n",
       " ('Subject', 'NN'),\n",
       " (':', ':'),\n",
       " ('Conference', 'NN'),\n",
       " ('on', 'IN'),\n",
       " ('Manned', 'NNP'),\n",
       " ('Lunar', 'NNP'),\n",
       " ('Exploration', 'NNP'),\n",
       " ('.', '.'),\n",
       " ('May', 'NNP'),\n",
       " ('7', 'CD'),\n",
       " ('Crystal', 'NNP'),\n",
       " ('City', 'NNP'),\n",
       " ('Organization', 'NNP'),\n",
       " (':', ':'),\n",
       " ('Express', 'NNP'),\n",
       " ('Access', 'NNP'),\n",
       " ('Online', 'NNP'),\n",
       " ('Communications', 'NNP'),\n",
       " (',', ','),\n",
       " ('Greenbelt', 'NNP'),\n",
       " ('MD', 'NNP'),\n",
       " ('USA', 'NNP'),\n",
       " ('Lines', 'NNPS'),\n",
       " (':', ':'),\n",
       " ('9', 'CD'),\n",
       " ('Distribution', 'NN'),\n",
       " (':', ':'),\n",
       " ('na', 'JJ'),\n",
       " ('NNTP-Posting-Host', 'NN'),\n",
       " (':', ':'),\n",
       " ('access.digex.net', 'NN'),\n",
       " ('AW', 'NNP'),\n",
       " ('&', 'CC'),\n",
       " ('ST', 'NNP'),\n",
       " ('had', 'VBD'),\n",
       " ('a', 'DT'),\n",
       " ('brief', 'JJ'),\n",
       " ('blurb', 'NN'),\n",
       " ('on', 'IN'),\n",
       " ('a', 'DT'),\n",
       " ('Manned', 'JJ'),\n",
       " ('Lunar', 'NNP'),\n",
       " ('Exploration', 'NNP'),\n",
       " ('confernce', 'NN'),\n",
       " ('May', 'NNP'),\n",
       " ('7th', 'CD'),\n",
       " ('at', 'IN'),\n",
       " ('Crystal', 'NNP'),\n",
       " ('City', 'NNP'),\n",
       " ('Virginia', 'NNP'),\n",
       " (',', ','),\n",
       " ('under', 'IN'),\n",
       " ('the', 'DT'),\n",
       " ('auspices', 'NNS'),\n",
       " ('of', 'IN'),\n",
       " ('AIAA', 'NNP'),\n",
       " ('.', '.'),\n",
       " ('Does', 'NNP'),\n",
       " ('anyone', 'NN'),\n",
       " ('know', 'VB'),\n",
       " ('more', 'JJR'),\n",
       " ('about', 'IN'),\n",
       " ('this', 'DT'),\n",
       " ('?', '.'),\n",
       " ('How', 'WRB'),\n",
       " ('much', 'RB'),\n",
       " (',', ','),\n",
       " ('to', 'TO'),\n",
       " ('attend', 'VB'),\n",
       " ('?', '.'),\n",
       " ('?', '.'),\n",
       " ('?', '.'),\n",
       " ('?', '.'),\n",
       " ('Anyone', 'NNP'),\n",
       " ('want', 'VBP'),\n",
       " ('to', 'TO'),\n",
       " ('go', 'VB'),\n",
       " ('?', '.'),\n",
       " ('pat', 'NN'),\n",
       " ('From', 'IN'),\n",
       " (':', ':'),\n",
       " ('Nanci', 'NNP'),\n",
       " ('Ann', 'NNP'),\n",
       " ('Miller', 'NNP'),\n",
       " ('<', 'NNP'),\n",
       " ('nm0w+', 'JJ'),\n",
       " ('@', 'NNP'),\n",
       " ('andrew.cmu.edu', 'NN'),\n",
       " ('>', 'NN'),\n",
       " ('Subject', 'NNP'),\n",
       " (':', ':'),\n",
       " ('Re', 'NN'),\n",
       " (':', ':'),\n",
       " ('Genocide', 'NNP'),\n",
       " ('is', 'VBZ'),\n",
       " ('Caused', 'VBN'),\n",
       " ('by', 'IN'),\n",
       " ('Atheism', 'NNP'),\n",
       " ('Organization', 'NNP'),\n",
       " (':', ':'),\n",
       " ('Sponsored', 'NNP'),\n",
       " ('account', 'NN'),\n",
       " (',', ','),\n",
       " ('School', 'NNP'),\n",
       " ('of', 'IN'),\n",
       " ('Computer', 'NNP'),\n",
       " ('Science', 'NNP'),\n",
       " (',', ','),\n",
       " ('Carnegie', 'NNP'),\n",
       " ('Mellon', 'NNP'),\n",
       " (',', ','),\n",
       " ('Pittsburgh', 'NNP'),\n",
       " (',', ','),\n",
       " ('PA', 'NNP'),\n",
       " ('Lines', 'NNPS'),\n",
       " (':', ':'),\n",
       " ('27', 'CD'),\n",
       " ('NNTP-Posting-Host', 'NN'),\n",
       " (':', ':'),\n",
       " ('andrew.cmu.edu', 'JJ'),\n",
       " ('In-Reply-To', 'NN'),\n",
       " (':', ':'),\n",
       " ('<', 'JJ'),\n",
       " ('1993Apr5.020504.19326', 'CD'),\n",
       " ('@', 'JJ'),\n",
       " ('ultb.isc.rit.edu', 'JJ'),\n",
       " ('>', 'NN'),\n",
       " ('snm6394', 'NN'),\n",
       " ('@', 'NNP'),\n",
       " ('ultb.isc.rit.edu', 'NN'),\n",
       " ('(', '('),\n",
       " ('S.N', 'NNP'),\n",
       " ('.', '.'),\n",
       " ('Mozumder', 'NNP'),\n",
       " (')', ')'),\n",
       " ('writes', 'VBZ'),\n",
       " (':', ':'),\n",
       " ('>', 'NN'),\n",
       " ('More', 'RBR'),\n",
       " ('horrible', 'JJ'),\n",
       " ('deaths', 'NNS'),\n",
       " ('resulted', 'VBD'),\n",
       " ('from', 'IN'),\n",
       " ('atheism', 'NN'),\n",
       " ('than', 'IN'),\n",
       " ('anything', 'NN'),\n",
       " ('else', 'RB'),\n",
       " ('.', '.'),\n",
       " ('There', 'EX'),\n",
       " ('are', 'VBP'),\n",
       " ('definitely', 'RB'),\n",
       " ('quite', 'RB'),\n",
       " ('a', 'DT'),\n",
       " ('few', 'JJ'),\n",
       " ('horrible', 'JJ'),\n",
       " ('deaths', 'NNS'),\n",
       " ('as', 'IN'),\n",
       " ('the', 'DT'),\n",
       " ('result', 'NN'),\n",
       " ('of', 'IN'),\n",
       " ('both', 'DT'),\n",
       " ('atheists', 'NNS'),\n",
       " ('AND', 'VBP'),\n",
       " ('theists', 'NNS'),\n",
       " ('.', '.'),\n",
       " ('I', 'PRP'),\n",
       " (\"'m\", 'VBP'),\n",
       " ('sure', 'JJ'),\n",
       " ('Bobby', 'NNP'),\n",
       " ('can', 'MD'),\n",
       " ('list', 'VB'),\n",
       " ('quite', 'RB'),\n",
       " ('a', 'DT'),\n",
       " ('few', 'JJ'),\n",
       " ('for', 'IN'),\n",
       " ('the', 'DT'),\n",
       " ('atheist', 'JJ'),\n",
       " ('side', 'NN'),\n",
       " ('but', 'CC'),\n",
       " ('fails', 'VBZ'),\n",
       " ('to', 'TO'),\n",
       " ('recognize', 'VB'),\n",
       " ('that', 'IN'),\n",
       " ('the', 'DT'),\n",
       " ('theists', 'NNS'),\n",
       " ('are', 'VBP'),\n",
       " ('equally', 'RB'),\n",
       " ('proficient', 'JJ'),\n",
       " ('at', 'IN'),\n",
       " ('genocide', 'NN'),\n",
       " ('.', '.'),\n",
       " ('Perhaps', 'RB'),\n",
       " (',', ','),\n",
       " ('since', 'IN'),\n",
       " ('I', 'PRP'),\n",
       " (\"'m\", 'VBP'),\n",
       " ('a', 'DT'),\n",
       " ('bit', 'NN'),\n",
       " ('weak', 'JJ'),\n",
       " ('on', 'IN'),\n",
       " ('history', 'NN'),\n",
       " (',', ','),\n",
       " ('somone', 'NN'),\n",
       " ...]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#POS tagging the words\n",
    "print(\"Number of words after Tokenization:\") \n",
    "print(len(words))\n",
    "print(\"Sample words after Tokenization:\")\n",
    "print(words[:30])\n",
    "print(\"Sample words after POS tagging:\")\n",
    "nltk.pos_tag(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dh5nxeZJaAq5"
   },
   "source": [
    "#### Frequency with Filter, PMI, T-test, Chi-Sq test\n",
    "> b. Apply the techniques described in Tutorial 6 (Frequency with filter, PMI, T-test with filter,\n",
    "Chi-Sq test) to extract bigram collocations from the corpus. Show the top 20 results of each\n",
    "technique. (15 marks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yy7nHCEScHCi"
   },
   "source": [
    "Frequency without filter\n",
    "> *  compare pairs of words (bigram where n=2 of n-grams)to see which pair is more likely to occur in the dataset -frequency\n",
    "* Frequency is calculated first without applying any filter or cleaning of dataset (ie.. with stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 686
    },
    "colab_type": "code",
    "id": "iKOku-LsaAq7",
    "outputId": "153ada60-4d7e-40db-c7cd-7f5abfcde0ee"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 20 bigrams as per frequencies without filter:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bigram</th>\n",
       "      <th>freq</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1132</th>\n",
       "      <td>(--, --)</td>\n",
       "      <td>12597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2107</th>\n",
       "      <td>(&gt;, &gt;)</td>\n",
       "      <td>5806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>534</th>\n",
       "      <td>(of, the)</td>\n",
       "      <td>3084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568</th>\n",
       "      <td>(,, and)</td>\n",
       "      <td>2619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(From, :)</td>\n",
       "      <td>2136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>(Subject, :)</td>\n",
       "      <td>2085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>271</th>\n",
       "      <td>(., &gt;)</td>\n",
       "      <td>2051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>(Lines, :)</td>\n",
       "      <td>2049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4250</th>\n",
       "      <td>(|, &gt;)</td>\n",
       "      <td>1979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>180</th>\n",
       "      <td>(Organization, :)</td>\n",
       "      <td>1950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1284</th>\n",
       "      <td>(., The)</td>\n",
       "      <td>1745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>(), Subject)</td>\n",
       "      <td>1742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>(in, the)</td>\n",
       "      <td>1738</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>(., I)</td>\n",
       "      <td>1734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202</th>\n",
       "      <td>(:, &gt;)</td>\n",
       "      <td>1689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159</th>\n",
       "      <td>(Re, :)</td>\n",
       "      <td>1599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201</th>\n",
       "      <td>(writes, :)</td>\n",
       "      <td>1544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1324</th>\n",
       "      <td>(,, but)</td>\n",
       "      <td>1489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158</th>\n",
       "      <td>(:, Re)</td>\n",
       "      <td>1462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200</th>\n",
       "      <td>(), writes)</td>\n",
       "      <td>1310</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 bigram   freq\n",
       "1132           (--, --)  12597\n",
       "2107             (>, >)   5806\n",
       "534           (of, the)   3084\n",
       "568            (,, and)   2619\n",
       "0             (From, :)   2136\n",
       "9          (Subject, :)   2085\n",
       "271              (., >)   2051\n",
       "21           (Lines, :)   2049\n",
       "4250             (|, >)   1979\n",
       "180   (Organization, :)   1950\n",
       "1284           (., The)   1745\n",
       "8          (), Subject)   1742\n",
       "87            (in, the)   1738\n",
       "102              (., I)   1734\n",
       "202              (:, >)   1689\n",
       "159             (Re, :)   1599\n",
       "201         (writes, :)   1544\n",
       "1324           (,, but)   1489\n",
       "158             (:, Re)   1462\n",
       "200         (), writes)   1310"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Frequency without filter\n",
    "from nltk.collocations import *\n",
    "bigramFinder = BigramCollocationFinder.from_words(words) # collocation identification\n",
    "bigram_freq = bigramFinder.ngram_fd.items()\n",
    "bigramFreqTable = pd.DataFrame(list(bigram_freq), columns=['bigram','freq']).sort_values(by='freq', ascending=False)\n",
    "\n",
    "print(\"Top 20 bigrams as per frequencies without filter:\")\n",
    "bigramFreqTable[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "crZLsMrAgATW"
   },
   "source": [
    "Frequency with filter\n",
    "> * Frequency calculated by removing stopwords in bigrams to get better result \n",
    "* Frequency calculated for only (Noun,Noun) and (Adjective,Noun)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "J8KjNRY9aArB"
   },
   "outputs": [],
   "source": [
    "#Frequency with filter\n",
    "from nltk.corpus import stopwords\n",
    "#get english stopwords\n",
    "en_stopwords = set(stopwords.words('english'))\n",
    "\n",
    "#function to filter out for ADJ/NN bigrams\n",
    "def rightTypes(ngram):\n",
    "    if '-pron-' in ngram or '' in ngram or ' 'in ngram or 't' in ngram:\n",
    "        return False\n",
    "    for word in ngram:\n",
    "        if word in en_stopwords:\n",
    "            return False\n",
    "    acceptable_types = ('JJ', 'JJR', 'JJS', 'NN', 'NNS', 'NNP', 'NNPS')\n",
    "    second_type = ('NN', 'NNS', 'NNP', 'NNPS')\n",
    "    tags = nltk.pos_tag(ngram)\n",
    "    if tags[0][1] in acceptable_types and tags[1][1] in second_type:\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 686
    },
    "colab_type": "code",
    "id": "rlbXvMZYaArE",
    "outputId": "568d1d93-c6d6-4437-8f6b-f47fb6186c98"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 20 bigrams as per frequencies with filter:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bigram</th>\n",
       "      <th>freq</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2107</th>\n",
       "      <td>(&gt;, &gt;)</td>\n",
       "      <td>5806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4250</th>\n",
       "      <td>(|, &gt;)</td>\n",
       "      <td>1979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190</th>\n",
       "      <td>(article, &lt;)</td>\n",
       "      <td>1126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4264</th>\n",
       "      <td>(&gt;, |)</td>\n",
       "      <td>685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1438</th>\n",
       "      <td>(|, |)</td>\n",
       "      <td>554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2658</th>\n",
       "      <td>(&gt;, &lt;)</td>\n",
       "      <td>137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12423</th>\n",
       "      <td>(*, *)</td>\n",
       "      <td>128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1726</th>\n",
       "      <td>(world, NNTP-Posting-Host)</td>\n",
       "      <td>128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6593</th>\n",
       "      <td>(@, newton.apple.com)</td>\n",
       "      <td>126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6592</th>\n",
       "      <td>(sandvik, @)</td>\n",
       "      <td>126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9533</th>\n",
       "      <td>(], &gt;)</td>\n",
       "      <td>124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1454</th>\n",
       "      <td>(@, zoo.toronto.edu)</td>\n",
       "      <td>117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8049</th>\n",
       "      <td>(@, cco.caltech.edu)</td>\n",
       "      <td>112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>709</th>\n",
       "      <td>(@, access.digex.com)</td>\n",
       "      <td>104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1457</th>\n",
       "      <td>(Henry, Spencer)</td>\n",
       "      <td>96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9225</th>\n",
       "      <td>(@, netcom.com)</td>\n",
       "      <td>96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12479</th>\n",
       "      <td>(ra, &gt;)</td>\n",
       "      <td>95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8048</th>\n",
       "      <td>(keith, @)</td>\n",
       "      <td>95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8029</th>\n",
       "      <td>(Jon, Livesey)</td>\n",
       "      <td>93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8025</th>\n",
       "      <td>(livesey, @)</td>\n",
       "      <td>92</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           bigram  freq\n",
       "2107                       (>, >)  5806\n",
       "4250                       (|, >)  1979\n",
       "190                  (article, <)  1126\n",
       "4264                       (>, |)   685\n",
       "1438                       (|, |)   554\n",
       "2658                       (>, <)   137\n",
       "12423                      (*, *)   128\n",
       "1726   (world, NNTP-Posting-Host)   128\n",
       "6593        (@, newton.apple.com)   126\n",
       "6592                 (sandvik, @)   126\n",
       "9533                       (], >)   124\n",
       "1454         (@, zoo.toronto.edu)   117\n",
       "8049         (@, cco.caltech.edu)   112\n",
       "709         (@, access.digex.com)   104\n",
       "1457             (Henry, Spencer)    96\n",
       "9225              (@, netcom.com)    96\n",
       "12479                     (ra, >)    95\n",
       "8048                   (keith, @)    95\n",
       "8029               (Jon, Livesey)    93\n",
       "8025                 (livesey, @)    92"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#filtered bigrams\n",
    "filtered_bi = bigramFreqTable[bigramFreqTable.bigram.map(lambda x: rightTypes(x))]\n",
    "print(\"Top 20 bigrams as per frequencies with filter:\")\n",
    "filtered_bi[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "x4cJ1HTzUMaM"
   },
   "source": [
    "PMI (Pointwise Mutual Information)\n",
    "\n",
    "> * Removed the bigrams with frequency less than 20 as they are not useful for analysis using *apply_freq_filter*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 686
    },
    "colab_type": "code",
    "id": "ejrT54aGaArL",
    "outputId": "baf4920b-13bc-4afd-8549-81826e4046b5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 20 bigrams as per PMI:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bigram</th>\n",
       "      <th>PMI</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(comme, aucun)</td>\n",
       "      <td>15.266065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(525-3684, Telos)</td>\n",
       "      <td>15.195676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(Steinn, Sigurdsson)</td>\n",
       "      <td>15.195676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(ISLAMIC, LAW)</td>\n",
       "      <td>15.128562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(M/S, 525-3684)</td>\n",
       "      <td>15.064432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>(fait, comme)</td>\n",
       "      <td>15.064432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>(sank, Manhattan)</td>\n",
       "      <td>14.997317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>(Carnegie, Mellon)</td>\n",
       "      <td>14.833106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>(Beam, Jockey)</td>\n",
       "      <td>14.730013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>(Bake, Timmons)</td>\n",
       "      <td>14.707575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>(Frequently, Asked)</td>\n",
       "      <td>14.630477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>(Chapel, Hill)</td>\n",
       "      <td>14.619702</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>(Brigham, Young)</td>\n",
       "      <td>14.479469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>(Los, Angeles)</td>\n",
       "      <td>14.418069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>(MY, REPLY)</td>\n",
       "      <td>14.394580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>(Cookamunga, Tourist)</td>\n",
       "      <td>14.378540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>(Resource, Listing)</td>\n",
       "      <td>14.370763</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>(fred, j)</td>\n",
       "      <td>14.318533</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>(Asked, Questions)</td>\n",
       "      <td>14.266065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>(et, al)</td>\n",
       "      <td>14.238461</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   bigram        PMI\n",
       "0          (comme, aucun)  15.266065\n",
       "1       (525-3684, Telos)  15.195676\n",
       "2    (Steinn, Sigurdsson)  15.195676\n",
       "3          (ISLAMIC, LAW)  15.128562\n",
       "4         (M/S, 525-3684)  15.064432\n",
       "5           (fait, comme)  15.064432\n",
       "6       (sank, Manhattan)  14.997317\n",
       "7      (Carnegie, Mellon)  14.833106\n",
       "8          (Beam, Jockey)  14.730013\n",
       "9         (Bake, Timmons)  14.707575\n",
       "10    (Frequently, Asked)  14.630477\n",
       "11         (Chapel, Hill)  14.619702\n",
       "12       (Brigham, Young)  14.479469\n",
       "13         (Los, Angeles)  14.418069\n",
       "14            (MY, REPLY)  14.394580\n",
       "15  (Cookamunga, Tourist)  14.378540\n",
       "16    (Resource, Listing)  14.370763\n",
       "17              (fred, j)  14.318533\n",
       "18     (Asked, Questions)  14.266065\n",
       "19               (et, al)  14.238461"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#PMI\n",
    "bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
    "#removes the bigrams which has frequency less than 20 as they are not useful for analysis\n",
    "bigramFinder.apply_freq_filter(20) \n",
    "bigramPMITable = pd.DataFrame(list(bigramFinder.score_ngrams(bigram_measures.pmi)), columns=['bigram','PMI']).sort_values(by='PMI', ascending=False)\n",
    "print(\"Top 20 bigrams as per PMI:\")\n",
    "bigramPMITable[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "73As-f1OUv6j"
   },
   "source": [
    "T-test with filter\n",
    "\n",
    "> * Applied  filter (rightTypes function) as done above in \"frequency with filter\" to remove stopwords in bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 686
    },
    "colab_type": "code",
    "id": "f8MFpEZUVDkt",
    "outputId": "c3bac791-fec9-491c-ffdf-028039161b28"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 20 bigrams as per T-test:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bigram</th>\n",
       "      <th>t</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(&gt;, &gt;)</td>\n",
       "      <td>67.793577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>(|, &gt;)</td>\n",
       "      <td>41.903078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>(article, &lt;)</td>\n",
       "      <td>33.448606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>(|, |)</td>\n",
       "      <td>22.661219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>(&gt;, |)</td>\n",
       "      <td>21.782338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203</th>\n",
       "      <td>(world, NNTP-Posting-Host)</td>\n",
       "      <td>11.281066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>204</th>\n",
       "      <td>(*, *)</td>\n",
       "      <td>11.276498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>211</th>\n",
       "      <td>(@, newton.apple.com)</td>\n",
       "      <td>11.115683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>212</th>\n",
       "      <td>(sandvik, @)</td>\n",
       "      <td>11.115683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>225</th>\n",
       "      <td>(@, zoo.toronto.edu)</td>\n",
       "      <td>10.711340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>240</th>\n",
       "      <td>(@, cco.caltech.edu)</td>\n",
       "      <td>10.479966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>266</th>\n",
       "      <td>(@, access.digex.com)</td>\n",
       "      <td>10.098748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>291</th>\n",
       "      <td>(Henry, Spencer)</td>\n",
       "      <td>9.796450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>295</th>\n",
       "      <td>(@, netcom.com)</td>\n",
       "      <td>9.702564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>298</th>\n",
       "      <td>(Jon, Livesey)</td>\n",
       "      <td>9.642051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>304</th>\n",
       "      <td>(keith, @)</td>\n",
       "      <td>9.590963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>317</th>\n",
       "      <td>(livesey, @)</td>\n",
       "      <td>9.493201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>319</th>\n",
       "      <td>(TIN, [)</td>\n",
       "      <td>9.473896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>322</th>\n",
       "      <td>(ra, &gt;)</td>\n",
       "      <td>9.463118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>323</th>\n",
       "      <td>(henry, @)</td>\n",
       "      <td>9.462749</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         bigram          t\n",
       "1                        (>, >)  67.793577\n",
       "8                        (|, >)  41.903078\n",
       "19                 (article, <)  33.448606\n",
       "34                       (|, |)  22.661219\n",
       "38                       (>, |)  21.782338\n",
       "203  (world, NNTP-Posting-Host)  11.281066\n",
       "204                      (*, *)  11.276498\n",
       "211       (@, newton.apple.com)  11.115683\n",
       "212                (sandvik, @)  11.115683\n",
       "225        (@, zoo.toronto.edu)  10.711340\n",
       "240        (@, cco.caltech.edu)  10.479966\n",
       "266       (@, access.digex.com)  10.098748\n",
       "291            (Henry, Spencer)   9.796450\n",
       "295             (@, netcom.com)   9.702564\n",
       "298              (Jon, Livesey)   9.642051\n",
       "304                  (keith, @)   9.590963\n",
       "317                (livesey, @)   9.493201\n",
       "319                    (TIN, [)   9.473896\n",
       "322                     (ra, >)   9.463118\n",
       "323                  (henry, @)   9.462749"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# T-test with filter\n",
    "bigramTtable = pd.DataFrame(list(bigramFinder.score_ngrams(bigram_measures.student_t)), columns=['bigram','t']).sort_values(by='t', ascending=False)\n",
    "# Applying same filter (rightTypes function) as done above in \"frequency with filter\" to remove stopwords in bigrams\n",
    "filteredT_bi = bigramTtable[bigramTtable.bigram.map(lambda x: rightTypes(x))]\n",
    "print(\"Top 20 bigrams as per T-test:\")\n",
    "filteredT_bi[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GwIZ5P61Var7"
   },
   "source": [
    "Chi-Sq test\n",
    "\n",
    ">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 686
    },
    "colab_type": "code",
    "id": "WtA8dkCYaArV",
    "outputId": "17f4ee1b-d09e-4962-9836-c41732aec7bc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 20 bigrams as per Chi-Sq test:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bigram</th>\n",
       "      <th>chi-sq</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(525-3684, Telos)</td>\n",
       "      <td>788086.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(Carnegie, Mellon)</td>\n",
       "      <td>788086.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(Cookamunga, Tourist)</td>\n",
       "      <td>788086.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(Steinn, Sigurdsson)</td>\n",
       "      <td>788086.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(comme, aucun)</td>\n",
       "      <td>788086.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>(VAX/VMS, VNEWS)</td>\n",
       "      <td>761369.287996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>(ISLAMIC, LAW)</td>\n",
       "      <td>752262.954520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>(VNEWS, 1.41)</td>\n",
       "      <td>746604.947174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>(Los, Angeles)</td>\n",
       "      <td>722409.416552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>(M/S, 525-3684)</td>\n",
       "      <td>719554.956473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>(Frequently, Asked)</td>\n",
       "      <td>710180.413690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>(Beam, Jockey)</td>\n",
       "      <td>706557.172325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>(sank, Manhattan)</td>\n",
       "      <td>686846.875418</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>(fait, comme)</td>\n",
       "      <td>685289.565151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>(Allan, Schneider)</td>\n",
       "      <td>658747.051132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>(Henry, Spencer)</td>\n",
       "      <td>623144.868326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>(Ron, Baalke)</td>\n",
       "      <td>622671.801378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>(Tourist, Bureau)</td>\n",
       "      <td>620400.254950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>(Jet, Propulsion)</td>\n",
       "      <td>608140.330727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>(Bake, Timmons)</td>\n",
       "      <td>588626.228221</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   bigram         chi-sq\n",
       "0       (525-3684, Telos)  788086.000000\n",
       "1      (Carnegie, Mellon)  788086.000000\n",
       "2   (Cookamunga, Tourist)  788086.000000\n",
       "3    (Steinn, Sigurdsson)  788086.000000\n",
       "4          (comme, aucun)  788086.000000\n",
       "5        (VAX/VMS, VNEWS)  761369.287996\n",
       "6          (ISLAMIC, LAW)  752262.954520\n",
       "7           (VNEWS, 1.41)  746604.947174\n",
       "8          (Los, Angeles)  722409.416552\n",
       "9         (M/S, 525-3684)  719554.956473\n",
       "10    (Frequently, Asked)  710180.413690\n",
       "11         (Beam, Jockey)  706557.172325\n",
       "12      (sank, Manhattan)  686846.875418\n",
       "13          (fait, comme)  685289.565151\n",
       "14     (Allan, Schneider)  658747.051132\n",
       "15       (Henry, Spencer)  623144.868326\n",
       "16          (Ron, Baalke)  622671.801378\n",
       "17      (Tourist, Bureau)  620400.254950\n",
       "18      (Jet, Propulsion)  608140.330727\n",
       "19        (Bake, Timmons)  588626.228221"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Chi-Sq test\n",
    "bigramChiTable = pd.DataFrame(list(bigramFinder.score_ngrams(bigram_measures.chi_sq)), columns=['bigram','chi-sq']).sort_values(by='chi-sq', ascending=False)\n",
    "print(\"Top 20 bigrams as per Chi-Sq test:\")\n",
    "bigramChiTable.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BY7hCoUiggjH"
   },
   "source": [
    "> * Sorting and arranging the index  for frequency with filter and filtered T-test as it is not starting from zero, which thereby avoids inconsistent results while displaying all tables together "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iVxuQkYXaArZ"
   },
   "outputs": [],
   "source": [
    "#Reset index of Frequency without filter\n",
    "#Ref: https://stackoverflow.com/questions/19609631/python-changing-row-index-of-pandas-data-frame\n",
    "sortedFrequencyWithFilter = filtered_bi[:20]\n",
    "sortedFrequencyWithFilter.index = range(20) \n",
    "sortedFilteredTtest = filteredT_bi[:20]\n",
    "sortedFilteredTtest.index=range(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NqD89eJHhoxU"
   },
   "source": [
    "> Concatenate all tabular results to compare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 669
    },
    "colab_type": "code",
    "id": "RoE25hRXaArc",
    "outputId": "63117ac5-2c67-4fbd-935a-5380c1e7eb23",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filtered_bi</th>\n",
       "      <th>bigramPMITable</th>\n",
       "      <th>bigramTtable</th>\n",
       "      <th>bigramChiTable</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(&gt;, &gt;)</td>\n",
       "      <td>(comme, aucun)</td>\n",
       "      <td>(&gt;, &gt;)</td>\n",
       "      <td>(525-3684, Telos)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(|, &gt;)</td>\n",
       "      <td>(525-3684, Telos)</td>\n",
       "      <td>(|, &gt;)</td>\n",
       "      <td>(Carnegie, Mellon)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(article, &lt;)</td>\n",
       "      <td>(Steinn, Sigurdsson)</td>\n",
       "      <td>(article, &lt;)</td>\n",
       "      <td>(Cookamunga, Tourist)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(&gt;, |)</td>\n",
       "      <td>(ISLAMIC, LAW)</td>\n",
       "      <td>(|, |)</td>\n",
       "      <td>(Steinn, Sigurdsson)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(|, |)</td>\n",
       "      <td>(M/S, 525-3684)</td>\n",
       "      <td>(&gt;, |)</td>\n",
       "      <td>(comme, aucun)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>(&gt;, &lt;)</td>\n",
       "      <td>(fait, comme)</td>\n",
       "      <td>(world, NNTP-Posting-Host)</td>\n",
       "      <td>(VAX/VMS, VNEWS)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>(*, *)</td>\n",
       "      <td>(sank, Manhattan)</td>\n",
       "      <td>(*, *)</td>\n",
       "      <td>(ISLAMIC, LAW)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>(world, NNTP-Posting-Host)</td>\n",
       "      <td>(Carnegie, Mellon)</td>\n",
       "      <td>(@, newton.apple.com)</td>\n",
       "      <td>(VNEWS, 1.41)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>(@, newton.apple.com)</td>\n",
       "      <td>(Beam, Jockey)</td>\n",
       "      <td>(sandvik, @)</td>\n",
       "      <td>(Los, Angeles)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>(sandvik, @)</td>\n",
       "      <td>(Bake, Timmons)</td>\n",
       "      <td>(@, zoo.toronto.edu)</td>\n",
       "      <td>(M/S, 525-3684)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>(], &gt;)</td>\n",
       "      <td>(Frequently, Asked)</td>\n",
       "      <td>(@, cco.caltech.edu)</td>\n",
       "      <td>(Frequently, Asked)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>(@, zoo.toronto.edu)</td>\n",
       "      <td>(Chapel, Hill)</td>\n",
       "      <td>(@, access.digex.com)</td>\n",
       "      <td>(Beam, Jockey)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>(@, cco.caltech.edu)</td>\n",
       "      <td>(Brigham, Young)</td>\n",
       "      <td>(Henry, Spencer)</td>\n",
       "      <td>(sank, Manhattan)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>(@, access.digex.com)</td>\n",
       "      <td>(Los, Angeles)</td>\n",
       "      <td>(@, netcom.com)</td>\n",
       "      <td>(fait, comme)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>(Henry, Spencer)</td>\n",
       "      <td>(MY, REPLY)</td>\n",
       "      <td>(Jon, Livesey)</td>\n",
       "      <td>(Allan, Schneider)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>(@, netcom.com)</td>\n",
       "      <td>(Cookamunga, Tourist)</td>\n",
       "      <td>(keith, @)</td>\n",
       "      <td>(Henry, Spencer)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>(ra, &gt;)</td>\n",
       "      <td>(Resource, Listing)</td>\n",
       "      <td>(livesey, @)</td>\n",
       "      <td>(Ron, Baalke)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>(keith, @)</td>\n",
       "      <td>(fred, j)</td>\n",
       "      <td>(TIN, [)</td>\n",
       "      <td>(Tourist, Bureau)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>(Jon, Livesey)</td>\n",
       "      <td>(Asked, Questions)</td>\n",
       "      <td>(ra, &gt;)</td>\n",
       "      <td>(Jet, Propulsion)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>(livesey, @)</td>\n",
       "      <td>(et, al)</td>\n",
       "      <td>(henry, @)</td>\n",
       "      <td>(Bake, Timmons)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   filtered_bi         bigramPMITable  \\\n",
       "0                       (>, >)         (comme, aucun)   \n",
       "1                       (|, >)      (525-3684, Telos)   \n",
       "2                 (article, <)   (Steinn, Sigurdsson)   \n",
       "3                       (>, |)         (ISLAMIC, LAW)   \n",
       "4                       (|, |)        (M/S, 525-3684)   \n",
       "5                       (>, <)          (fait, comme)   \n",
       "6                       (*, *)      (sank, Manhattan)   \n",
       "7   (world, NNTP-Posting-Host)     (Carnegie, Mellon)   \n",
       "8        (@, newton.apple.com)         (Beam, Jockey)   \n",
       "9                 (sandvik, @)        (Bake, Timmons)   \n",
       "10                      (], >)    (Frequently, Asked)   \n",
       "11        (@, zoo.toronto.edu)         (Chapel, Hill)   \n",
       "12        (@, cco.caltech.edu)       (Brigham, Young)   \n",
       "13       (@, access.digex.com)         (Los, Angeles)   \n",
       "14            (Henry, Spencer)            (MY, REPLY)   \n",
       "15             (@, netcom.com)  (Cookamunga, Tourist)   \n",
       "16                     (ra, >)    (Resource, Listing)   \n",
       "17                  (keith, @)              (fred, j)   \n",
       "18              (Jon, Livesey)     (Asked, Questions)   \n",
       "19                (livesey, @)               (et, al)   \n",
       "\n",
       "                  bigramTtable         bigramChiTable  \n",
       "0                       (>, >)      (525-3684, Telos)  \n",
       "1                       (|, >)     (Carnegie, Mellon)  \n",
       "2                 (article, <)  (Cookamunga, Tourist)  \n",
       "3                       (|, |)   (Steinn, Sigurdsson)  \n",
       "4                       (>, |)         (comme, aucun)  \n",
       "5   (world, NNTP-Posting-Host)       (VAX/VMS, VNEWS)  \n",
       "6                       (*, *)         (ISLAMIC, LAW)  \n",
       "7        (@, newton.apple.com)          (VNEWS, 1.41)  \n",
       "8                 (sandvik, @)         (Los, Angeles)  \n",
       "9         (@, zoo.toronto.edu)        (M/S, 525-3684)  \n",
       "10        (@, cco.caltech.edu)    (Frequently, Asked)  \n",
       "11       (@, access.digex.com)         (Beam, Jockey)  \n",
       "12            (Henry, Spencer)      (sank, Manhattan)  \n",
       "13             (@, netcom.com)          (fait, comme)  \n",
       "14              (Jon, Livesey)     (Allan, Schneider)  \n",
       "15                  (keith, @)       (Henry, Spencer)  \n",
       "16                (livesey, @)          (Ron, Baalke)  \n",
       "17                    (TIN, [)      (Tourist, Bureau)  \n",
       "18                     (ra, >)      (Jet, Propulsion)  \n",
       "19                  (henry, @)        (Bake, Timmons)  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Concatenate all tables to compare each other\n",
    "comp=pd.concat([sortedFrequencyWithFilter['bigram'], bigramPMITable['bigram'],sortedFilteredTtest['bigram'],bigramChiTable['bigram']],axis=1, keys=['filtered_bi', 'bigramPMITable','bigramTtable','bigramChiTable'])\n",
    "comp.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FZ8EKJCv9MID"
   },
   "source": [
    "#### Justification to consider overlap of techniques\n",
    "\n",
    "> c. How much overlap is there among the techniques? Do you think it makes sense to consider\n",
    "the union of the results?\n",
    "\n",
    "\n",
    "* Chi-Sq test, PMI - gives probability lift or term frequency\n",
    "* Frequency with filter and T-test gives frequency of occurence\n",
    "* By analysing and comparing all the tabular results, it seems like results of frequency with filter and T-test with filter is almost similar while PMI results is similar to Chi-Sq test. \n",
    "* On strict comparision we can see that PMI and Chi-Sq has better results (ie..alphabetic variables) without even applying filters. \n",
    "* But Chi-Sq test and PMI results give relationship between two variables but not strength or size of the relationship. \n",
    "* So, in my perspective if we consider union of term frequency (probability lift) and frequency of occurency, we get better results  as both size and relationship of variables will be taken into account instead of combining all the four techniques which might give inconsistent results. \n",
    "* In other words, we should either combine PMI and frequency with filter or PMI and T-test or Chi-Sq and frequency with filter or Chi-Sq and T-test for better results.\n",
    "\n",
    "**References**:\n",
    "1. https://medium.com/@nicharuch/collocations-identifying-phrases-that-act-like-individual-wordsin-nlp-f58a93a2f84ab\n",
    "2. https://bib.irb.hr/datoteka/251298.110-4-157-203.pdf\n",
    "3. http://www.site.uottawa.ca/~diana/csi5180/L7.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ExX3cCttaArg"
   },
   "source": [
    "#### Q2. SVM and NB for Text Classification (75 marks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wvPrYnKXaArh"
   },
   "source": [
    "> a. Clean the text: ( 10 marks )\n",
    " * Remove stop words\n",
    " * Remove numbers and other non-letter characters\n",
    " * Stem the words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "D45Rg5Z22cMl"
   },
   "source": [
    "> Tokenizing the given dataset of documents\n",
    "* We are converting the dataset into array of documents instead of single array of words as Tf-Idf calculates score relative to each document.\n",
    "*  We need to find bag of words as part of feature extraction, so first need to remove unimportant tokens in the documents and clean the dataset.\n",
    "* To identify stopwords, which are of lowercase in NLTK library, we need to convert all words of document to lowercase and then try to remove them.\n",
    "\n",
    "\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "W-SowK1XaArh"
   },
   "outputs": [],
   "source": [
    "#Tokeinze the documents\n",
    "newsgroups = newsgroups_train.data\n",
    "tokenizedWords=[]*len(newsgroups)\n",
    "for i in range(len(newsgroups)):\n",
    "    tokenizedWords.append(word_tokenize(newsgroups[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OXpmKbxEaArl"
   },
   "outputs": [],
   "source": [
    "#Converting all into lower case words before removing stop words\n",
    "for i in range(len(tokenizedWords)):\n",
    "    for j in range(len(tokenizedWords[i])):\n",
    "        tokenizedWords[i][j] = tokenizedWords[i][j].lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GYzZFBLLaAro"
   },
   "outputs": [],
   "source": [
    "#Remove Stop words\n",
    "#Get all stop words from NLTK\n",
    "#Ref: https://www.geeksforgeeks.org/removing-stop-words-nltk-python/\n",
    "stop_words = set(stopwords.words('english')) \n",
    "\n",
    "for i in range(len(tokenizedWords)):\n",
    "    for j in range(len(tokenizedWords[i])):\n",
    "        tokenizedWords[i] = [w for w in tokenizedWords[i] if not w in stop_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "n5GGk6avaAru"
   },
   "outputs": [],
   "source": [
    "#Remove numbers and other non-letter characters\n",
    "for i in range(len(tokenizedWords)):\n",
    "    for j in range(len(tokenizedWords[i])):\n",
    "        tokenizedWords[i] = [w for w in tokenizedWords[i] if w.isalpha()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KbT5wUJgaArx"
   },
   "outputs": [],
   "source": [
    "#Stem the words\n",
    "#Ref: Lab Text Processing\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "ps = PorterStemmer()\n",
    "\n",
    "for i in range(len(tokenizedWords)):\n",
    "    for j in range(len(tokenizedWords[i])):\n",
    "        tokenizedWords[i][j] = ps.stem(tokenizedWords[i][j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sHsfSD6KaAr1"
   },
   "outputs": [],
   "source": [
    "#Converting back the cleaned tokens into to String i.e, documents \n",
    "#Ref: https://www.decalage.info/en/python/print_list\n",
    "cleanedDocuments = [] \n",
    "for i in range(len(tokenizedWords)):\n",
    "        joinedString  = ' '.join(tokenizedWords[i])\n",
    "        cleanedDocuments.append(joinedString)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "subject biblic back koresh tape cite enclos kmcvay ken mcvay organ old frog almanac line articl stephen write seem koresh yet anoth messeng got kill messag carri say noth seem bar evid contrari koresh simpli anoth derang fanat thought neccessari take whole bunch folk children satisfi delusion mania jim jone circa mean time sure learn lot evil corrupt surpris thing gotten rotten nope fruitcak like koresh demonstr evil corrupt centuri old frog almanac salut old frog hiss f ryugen fisher sco xenix gt ladysmith british columbia canada serv central vancouv island public access usenet internet mail home holocaust almanac\n"
     ]
    }
   ],
   "source": [
    "#Sample cleaned Documemnt without Stop words, numbers and other non-letter characters and stemming done.\n",
    "print(cleanedDocuments[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nuDZGcisaAr3"
   },
   "source": [
    "#### Tf-Idf (Term Frequency- Inverse Document Frequency)weighted vector representation\n",
    "> b. Study the section on feature extraction in Scikit Learn,  and convert the corpus into a bag-of-words tf-idf weighted vector representation\n",
    "\n",
    "Ref: https://scikit-learn.org/stable/modules/feature_extraction.html#text-feature-extraction\n",
    " \n",
    ">  Two methods to  perform feature extraction from text files\n",
    "* Count Vectorizer + Tf-IDF Transformer\n",
    "* Tf-IDF Vectorizer\n",
    "\n",
    "Ref: http://kavita-ganesan.com/tfidftransformer-tfidfvectorizer-usage-differences/#.XS8mEuhKjIU\n",
    "Ref: https://scikit-learn.org/stable/modules/feature_extraction.html#text-feature-extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ik9bkMTa86PT"
   },
   "source": [
    "##### Count Vectorizer + Tf-IDF Transformer\n",
    "\n",
    "> Count Vectroiser is used to determine the feartures i.e, the bag of words for each document in the corpus and then find the counts of each word in the document. As documents may have huge number of words which results in a massive matirx. Count Vectoriser uses sparse representation of the matrix, which reduces the overhead in representing it in a massive matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "HJuymNexaAr4",
    "outputId": "03bdf8c3-e568-4743-9cd2-22c9210266cb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2034, 18399)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Count Vectorizing\n",
    "#Ref: Lab\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "count_vect = CountVectorizer()\n",
    "X_train_counts = count_vect.fit_transform(cleanedDocuments)\n",
    "X_train_counts.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ktf1u9o6OgNg"
   },
   "source": [
    "> TF-IDF Transformer computes IDF values for the array of documents in the dataset\n",
    "IDF values calculates the score for each word(term) based on their appearance in each to every document in our collection,thereby normalises the importance of tokens in the document( ie.., balance the dataset as unimportant tokens with high term frequency might grade down the important tokens term frequency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "UUX7DpqXaAr8",
    "outputId": "b6ceb93f-8319-426e-c5c8-1f8b44ec4b8f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2034, 18399)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#TfidfTransformer \n",
    "#Ref: Lab\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)\n",
    "X_train_tfidf.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IndDqQ2LSWtA"
   },
   "source": [
    "> Displaying the TF_IDF scores for the first document :\n",
    "* The more common the word across documents, the lower its score and the more unique a word is to our first document the higher the score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "P2GfM7VKNChi",
    "outputId": "f2e2416e-1940-4b37-e4d6-8929245b3fe4"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tfidf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>rych</th>\n",
       "      <td>0.457032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>file</th>\n",
       "      <td>0.382339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>textur</th>\n",
       "      <td>0.337113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>save</th>\n",
       "      <td>0.234388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hawk</th>\n",
       "      <td>0.232725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rule</th>\n",
       "      <td>0.226992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>format</th>\n",
       "      <td>0.157913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rychard</th>\n",
       "      <td>0.152344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>restart</th>\n",
       "      <td>0.152344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>posit</th>\n",
       "      <td>0.142184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>reload</th>\n",
       "      <td>0.139020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>edinburgh</th>\n",
       "      <td>0.139020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>inform</th>\n",
       "      <td>0.126502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>preserv</th>\n",
       "      <td>0.111206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>default</th>\n",
       "      <td>0.110107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>manual</th>\n",
       "      <td>0.105395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>psycholog</th>\n",
       "      <td>0.103792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>anyon</th>\n",
       "      <td>0.103063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>explicitli</th>\n",
       "      <td>0.102313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>store</th>\n",
       "      <td>0.100939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tel</th>\n",
       "      <td>0.096783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>orient</th>\n",
       "      <td>0.094283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>univ</th>\n",
       "      <td>0.093819</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>virtual</th>\n",
       "      <td>0.093819</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>plane</th>\n",
       "      <td>0.092493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>notic</th>\n",
       "      <td>0.087949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>environ</th>\n",
       "      <td>0.087615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>somewher</th>\n",
       "      <td>0.087286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>map</th>\n",
       "      <td>0.087286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>laboratori</th>\n",
       "      <td>0.079820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gall</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>galilleo</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>galileo</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>galilean</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>galbraith</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>garden</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>galaxi</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>galatian</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>galahad</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>galacticentr</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>galact</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gal</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>galleri</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gallileo</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gambit</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>game</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gamepro</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gamma</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ganador</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gandalf</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gandhi</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gang</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ganymed</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gao</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gap</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gara</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>garbag</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>garch</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>garcia</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nd</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>18399 rows  1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 tfidf\n",
       "rych          0.457032\n",
       "file          0.382339\n",
       "textur        0.337113\n",
       "save          0.234388\n",
       "hawk          0.232725\n",
       "rule          0.226992\n",
       "format        0.157913\n",
       "rychard       0.152344\n",
       "restart       0.152344\n",
       "posit         0.142184\n",
       "reload        0.139020\n",
       "edinburgh     0.139020\n",
       "inform        0.126502\n",
       "preserv       0.111206\n",
       "default       0.110107\n",
       "manual        0.105395\n",
       "psycholog     0.103792\n",
       "anyon         0.103063\n",
       "explicitli    0.102313\n",
       "store         0.100939\n",
       "tel           0.096783\n",
       "orient        0.094283\n",
       "univ          0.093819\n",
       "virtual       0.093819\n",
       "plane         0.092493\n",
       "notic         0.087949\n",
       "environ       0.087615\n",
       "somewher      0.087286\n",
       "map           0.087286\n",
       "laboratori    0.079820\n",
       "...                ...\n",
       "gall          0.000000\n",
       "galilleo      0.000000\n",
       "galileo       0.000000\n",
       "galilean      0.000000\n",
       "galbraith     0.000000\n",
       "garden        0.000000\n",
       "galaxi        0.000000\n",
       "galatian      0.000000\n",
       "galahad       0.000000\n",
       "galacticentr  0.000000\n",
       "galact        0.000000\n",
       "gal           0.000000\n",
       "galleri       0.000000\n",
       "gallileo      0.000000\n",
       "gambit        0.000000\n",
       "game          0.000000\n",
       "gamepro       0.000000\n",
       "gamma         0.000000\n",
       "ganador       0.000000\n",
       "gandalf       0.000000\n",
       "gandhi        0.000000\n",
       "gang          0.000000\n",
       "ganymed       0.000000\n",
       "gao           0.000000\n",
       "gap           0.000000\n",
       "gara          0.000000\n",
       "garbag        0.000000\n",
       "garch         0.000000\n",
       "garcia        0.000000\n",
       "nd           0.000000\n",
       "\n",
       "[18399 rows x 1 columns]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_names = count_vect.get_feature_names()\n",
    " \n",
    "#tfidf values for first document\n",
    "first_document_vector=X_train_tfidf[0]\n",
    " \n",
    "#print the scores\n",
    "df = pd.DataFrame(first_document_vector.T.todense(), index=feature_names, columns=[\"tfidf\"])\n",
    "df.sort_values(by=[\"tfidf\"],ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vrWpVBslSY_Z"
   },
   "source": [
    "#### TF-IDF Vectorizer\n",
    "\n",
    "> In TF-IDF Vectorizer all the steps done as part of CountVectorizer and TF-IDF transformer is done in a single step (ie.., both term frequency and normalisation of tokens in document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "YllNPgUpaAsA",
    "outputId": "69d24b36-3733-406b-87d0-075586c7699b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2034, 18399)\n"
     ]
    }
   ],
   "source": [
    "#Using TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(cleanedDocuments)\n",
    "#print(vectorizer.get_feature_names())\n",
    "print(X.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WgsBSO-hXpAM"
   },
   "source": [
    "> Printing the first document TF_IDF values calculated using TF_IDF vectorizer\n",
    "* We can observe the values are same as the values calculated using CountVectorizer+TF-IDF Transformer. So, we can use any one of them based on the requirements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "Z1UjuXG5TK2P",
    "outputId": "ff9a0059-a041-4ea3-c73d-96f34e0425b1"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tfidf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>rych</th>\n",
       "      <td>0.457032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>file</th>\n",
       "      <td>0.382339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>textur</th>\n",
       "      <td>0.337113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>save</th>\n",
       "      <td>0.234388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hawk</th>\n",
       "      <td>0.232725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rule</th>\n",
       "      <td>0.226992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>format</th>\n",
       "      <td>0.157913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rychard</th>\n",
       "      <td>0.152344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>restart</th>\n",
       "      <td>0.152344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>posit</th>\n",
       "      <td>0.142184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>reload</th>\n",
       "      <td>0.139020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>edinburgh</th>\n",
       "      <td>0.139020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>inform</th>\n",
       "      <td>0.126502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>preserv</th>\n",
       "      <td>0.111206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>default</th>\n",
       "      <td>0.110107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>manual</th>\n",
       "      <td>0.105395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>psycholog</th>\n",
       "      <td>0.103792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>anyon</th>\n",
       "      <td>0.103063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>explicitli</th>\n",
       "      <td>0.102313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>store</th>\n",
       "      <td>0.100939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tel</th>\n",
       "      <td>0.096783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>orient</th>\n",
       "      <td>0.094283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>univ</th>\n",
       "      <td>0.093819</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>virtual</th>\n",
       "      <td>0.093819</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>plane</th>\n",
       "      <td>0.092493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>notic</th>\n",
       "      <td>0.087949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>environ</th>\n",
       "      <td>0.087615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>somewher</th>\n",
       "      <td>0.087286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>map</th>\n",
       "      <td>0.087286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>laboratori</th>\n",
       "      <td>0.079820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gall</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>galilleo</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>galileo</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>galilean</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>galbraith</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>garden</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>galaxi</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>galatian</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>galahad</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>galacticentr</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>galact</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gal</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>galleri</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gallileo</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gambit</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>game</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gamepro</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gamma</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ganador</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gandalf</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gandhi</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gang</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ganymed</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gao</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gap</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gara</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>garbag</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>garch</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>garcia</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nd</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>18399 rows  1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 tfidf\n",
       "rych          0.457032\n",
       "file          0.382339\n",
       "textur        0.337113\n",
       "save          0.234388\n",
       "hawk          0.232725\n",
       "rule          0.226992\n",
       "format        0.157913\n",
       "rychard       0.152344\n",
       "restart       0.152344\n",
       "posit         0.142184\n",
       "reload        0.139020\n",
       "edinburgh     0.139020\n",
       "inform        0.126502\n",
       "preserv       0.111206\n",
       "default       0.110107\n",
       "manual        0.105395\n",
       "psycholog     0.103792\n",
       "anyon         0.103063\n",
       "explicitli    0.102313\n",
       "store         0.100939\n",
       "tel           0.096783\n",
       "orient        0.094283\n",
       "univ          0.093819\n",
       "virtual       0.093819\n",
       "plane         0.092493\n",
       "notic         0.087949\n",
       "environ       0.087615\n",
       "somewher      0.087286\n",
       "map           0.087286\n",
       "laboratori    0.079820\n",
       "...                ...\n",
       "gall          0.000000\n",
       "galilleo      0.000000\n",
       "galileo       0.000000\n",
       "galilean      0.000000\n",
       "galbraith     0.000000\n",
       "garden        0.000000\n",
       "galaxi        0.000000\n",
       "galatian      0.000000\n",
       "galahad       0.000000\n",
       "galacticentr  0.000000\n",
       "galact        0.000000\n",
       "gal           0.000000\n",
       "galleri       0.000000\n",
       "gallileo      0.000000\n",
       "gambit        0.000000\n",
       "game          0.000000\n",
       "gamepro       0.000000\n",
       "gamma         0.000000\n",
       "ganador       0.000000\n",
       "gandalf       0.000000\n",
       "gandhi        0.000000\n",
       "gang          0.000000\n",
       "ganymed       0.000000\n",
       "gao           0.000000\n",
       "gap           0.000000\n",
       "gara          0.000000\n",
       "garbag        0.000000\n",
       "garch         0.000000\n",
       "garcia        0.000000\n",
       "nd           0.000000\n",
       "\n",
       "[18399 rows x 1 columns]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " #first document TF_IDF values\n",
    "first_vector_tfidfvectorizer=X[0]\n",
    " \n",
    "# Print the values\n",
    "df = pd.DataFrame(first_vector_tfidfvectorizer.T.todense(), index=vectorizer.get_feature_names(), columns=[\"tfidf\"])\n",
    "df.sort_values(by=[\"tfidf\"],ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-MmumA7zTLqe"
   },
   "source": [
    "#### Tfidftransformer vs. Tfidfvectorizer\n",
    "Ref: http://kavita-ganesan.com/tfidftransformer-tfidfvectorizer-usage-differences/#.XS8mEuhKjIU\n",
    "\n",
    "\n",
    "> * TF-IDF tranformer is systematic stepwise process, as it starts with Countvectorizer which does tokenisation, removal of stopwords internally and then transformer normalises the tokens by calculating inverse document frequency.\n",
    "* Tf-IDf Vectorizer does everything together (word counts, IDF values, and Tf-idf scores)\n",
    "* If we focus on calculating term frequency better to use TF-IDF transformer or if the focus is calculating TF_IDF values then TF_IDF vectorizes serves the purpose in easier way."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "S4TcY153aAsC"
   },
   "source": [
    "#### c. Split the data randomly into training and testing set (70-30 %). ( 5 marks )\n",
    "* Training the classifier using training data\n",
    "* Calculate performance of classifier with the help of test data\n",
    "* Report Confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "K9Zm_gtfaAsD"
   },
   "outputs": [],
   "source": [
    "#Spliting Test-Train classification of dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "train_X,test_X,train_Y,test_Y=train_test_split(X,newsgroups_train.target,test_size=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tHmMyYZiaAsE"
   },
   "source": [
    "#### Train Multinomial NB and report confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QkJTSO--aAsF"
   },
   "outputs": [],
   "source": [
    "#feeding training data to model\n",
    "import numpy as np\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "clf = MultinomialNB().fit(train_X, train_Y)\n",
    "# performance test of classifier using test data\n",
    "predicted = clf.predict(test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "y2LDHTUUaAsI",
    "outputId": "d95c8b63-3ae5-4583-d97e-4ae18c5bb06f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score : 86.41571194762683\n",
      "Confusion Matrix :\n",
      "[[132   1   1   0]\n",
      " [  0 168   2   0]\n",
      " [  0   0 174   0]\n",
      " [ 64   2  13  54]]\n"
     ]
    }
   ],
   "source": [
    "# Accuracy and Confusion Matrix\n",
    "import numpy as np\n",
    "print(\"Accuracy Score :\",accuracy_score(predicted, test_Y)*100)\n",
    "print(\"Confusion Matrix :\")\n",
    "print(confusion_matrix(test_Y, predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2oXhcgqHaAsL"
   },
   "source": [
    "#### Train SVM and report confusion matrix\n",
    "> kernel types:\n",
    "* Linear\n",
    "* Poly\n",
    "* RBF (Radial Basis Function)\n",
    "* Sigmoid\n",
    "\n",
    "Ref: https://medium.com/@bedigunjit/simple-guide-to-text-classification-nlp-using-svm-and-naive-bayes-with-python-421db3a72d34\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yIxVlksr5hDj"
   },
   "source": [
    "> #### Kernel - Linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zNGiUWJ1aAsN",
    "outputId": "e0593236-e871-4a01-8fcb-5af036814b6e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score : 95.5810147299509\n",
      "Confusion Matrix :\n",
      "[[129   1   1   3]\n",
      " [  0 170   0   0]\n",
      " [  0   5 168   1]\n",
      " [  9   5   2 117]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "SVM = svm.SVC(C=1.0, kernel='linear', degree=3, gamma=2)\n",
    "SVM.fit(train_X, train_Y)\n",
    "# predict the labels on validation dataset\n",
    "predictions_SVM = SVM.predict(test_X)\n",
    "# Use accuracy_score function to get the accuracy\n",
    "print(\"Accuracy Score :\",accuracy_score(predictions_SVM, test_Y)*100)\n",
    "print(\"Confusion Matrix :\")\n",
    "print(confusion_matrix(test_Y, predictions_SVM))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ma8_h8ld5qYi"
   },
   "source": [
    "> #### Kernel - Poly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "W8zupJzjaAsP",
    "outputId": "0d34d9c7-8461-4343-b508-79fdf310efb7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score : 77.74140752864157\n",
      "Confusion Matrix :\n",
      "[[103  27   1   3]\n",
      " [  0 170   0   0]\n",
      " [  0  19 155   0]\n",
      " [  9  77   0  47]]\n"
     ]
    }
   ],
   "source": [
    "SVMpoly = svm.SVC(C=1.0, kernel='poly', degree=3, gamma=2)\n",
    "SVMpoly.fit(train_X, train_Y)\n",
    "# predict the labels on validation dataset\n",
    "predictions_poly = SVMpoly.predict(test_X)\n",
    "print(\"Accuracy Score :\",accuracy_score(predictions_poly, test_Y)*100)\n",
    "print(\"Confusion Matrix :\")\n",
    "print(confusion_matrix(test_Y, predictions_poly))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "C28N-pdr5ySV"
   },
   "source": [
    "> #### Kernel - rbf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vA3jZUChaAsS",
    "outputId": "ab156afd-ea21-485a-ca06-59229cb08ebd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score : 88.21603927986907\n",
      "Confusion Matrix :\n",
      "[[124   4   3   3]\n",
      " [  0 170   0   0]\n",
      " [  0   5 169   0]\n",
      " [ 17  27  13  76]]\n"
     ]
    }
   ],
   "source": [
    "SVM = svm.SVC(C=1.0, kernel='rbf', degree=3, gamma=2)\n",
    "SVM.fit(train_X, train_Y)\n",
    "# predict the labels on validation dataset\n",
    "predictions_rbf = SVM.predict(test_X)\n",
    "# Use accuracy_score function to get the accuracy\n",
    "predictions_poly = SVMpoly.predict(test_X)\n",
    "print(\"Accuracy Score :\",accuracy_score(predictions_rbf, test_Y)*100)\n",
    "print(\"Confusion Matrix :\")\n",
    "print(confusion_matrix(test_Y, predictions_rbf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CSY79oj853WO"
   },
   "source": [
    "> #### Kernel - Sigmoid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yaQ6mOJhaAsW",
    "outputId": "18a89e8e-8786-4870-a78e-f964d8eceec4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score : 95.5810147299509\n",
      "Confusion Matrix :\n",
      "[[129   1   1   3]\n",
      " [  0 170   0   0]\n",
      " [  1   6 167   0]\n",
      " [ 10   3   2 118]]\n"
     ]
    }
   ],
   "source": [
    "SVM = svm.SVC(C=1.0, kernel='sigmoid', degree=3, gamma=2)\n",
    "SVM.fit(train_X, train_Y)\n",
    "# predict the labels on validation dataset\n",
    "predictions_sig = SVM.predict(test_X)\n",
    "# Use accuracy_score function to get the accuracy\n",
    "predictions_poly = SVMpoly.predict(test_X)\n",
    "print(\"Accuracy Score :\",accuracy_score(predictions_sig, test_Y)*100)\n",
    "print(\"Confusion Matrix :\")\n",
    "print(confusion_matrix(test_Y, predictions_sig))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wb8TSCRcaAsd"
   },
   "source": [
    "#### Which algorithm has higher accuracy and why?\n",
    "\n",
    "> References: \n",
    "1. https://pdfs.semanticscholar.org/55c5/9874114617b57eedd8636c5d0e7785fb885f.pdf\n",
    "2. https://stackoverflow.com/questions/35360081/naive-bayes-vs-svm-for-classifying-text-data\n",
    "3. https://stats.stackexchange.com/questions/136538/is-the-naive-bayes-family-of-classifiers-linear\n",
    "\n",
    "> From our observation and datset, SVM  is performing better than Multinomial NB. But, the results clearly depends on the usage of dataset, cleaning, tuning parameters.\n",
    "* NB is generally stated as linear classifier (by not taking account of special cases). So, if datset has non-linearly separable data or noisy data, NB performs poor results compared to SVM. In theory, SVM  maps and represents data in higher dimensional hyperplans to perform better results even though dataset consists of non-linear data or noise.\n",
    "* In NB, featured are treated independently, whereas SVM gives importance to the relation between features\n",
    "* So, if the focus is on features and their interaction or if the dataset consists of little bit of noise and non-linear seperable data then SVM gives better results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0gDCQ5cuaAse"
   },
   "source": [
    "#### Does changing the kernel of the SVM change the accuracy or decrease confusion between classes?\n",
    "\n",
    "> Ref: \n",
    "1. https://www.quora.com/When-can-I-use-Linear-SVM-instead-of-RBF-polynomial-or-a-sigmoid-kernel\n",
    "2. https://pdfs.semanticscholar.org/55c5/9874114617b57eedd8636c5d0e7785fb885f.pdf\n",
    "3. https://www.kdnuggets.com/2016/06/select-support-vector-machine-kernels.html\n",
    "\n",
    "> Yes. Kernel plays a major role in the results. So, choosing proper kernel function is much needed in SVM and it depends on the type of dataset we are working.\n",
    "Here in our dataset, as we cleaned and removed the noise Linear kernel performed better. As Linear performed we can assume the dataset is not having much non-linearly seperable samples.\n",
    "* Linear: Suitable if datset is properly tuned, having large number of features without noise, and if there is no necessity of mapping features in high dimensional space (ie.. no non linear data in the datset)\n",
    "* Non-Linear:\n",
    "   * Polynomial: Similar to linear with arbitary boundaries. Suitable for non-linear data mapping without noise in the datset.\n",
    "   * RBF: More Suitable for dataset having less number of features and even though the datset has attributes in non-linear manner unlike linear kernel or multinomial NB. It is the best kernel of SVM and  serves the purpose which cannot be done by other kernels or ordinary NB. But, if the dataset is large it is expensive and complex to implement.\n",
    "   * Sigmoid: It is generally used in Neural networks and still in research phase which \n",
    "* Based on the complexity of dataset,expenses, noise, non- linear data in dataset we should choose the kernel types.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dJSDtmsbaAse"
   },
   "source": [
    "#### Perform part-of-speech tagging on the raw data (i.e. prior to cleaning it),"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "XVy8dlA8tPdS",
    "outputId": "1470bc98-8e40-49a8-86c4-3da920950ed0"
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "interest = ['alt.atheism', 'talk.religion.misc','comp.graphics','sci.space',]\n",
    "newsgroups_raw = fetch_20newsgroups(subset='train',categories=interest,shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "E_eEZs0i3Hg3"
   },
   "source": [
    "> #### Tokenizing Raw data for POS tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fdh_XMyctPdV"
   },
   "outputs": [],
   "source": [
    "#Tokenizing Raw data for POS tagging\n",
    "import nltk\n",
    "import pandas as pd\n",
    "from nltk import pos_tag\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "rawWords=[]*len(newsgroups_raw.data)\n",
    "for i in range(len(newsgroups_raw.data)):\n",
    "    rawWords.append(word_tokenize(newsgroups_raw.data[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nnF_kdXJ3Owe"
   },
   "source": [
    "> #### POS tagging on raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kkDvx90ctPdW"
   },
   "outputs": [],
   "source": [
    "#POS tagging on raw data before cleaning\n",
    "for i in range(len(rawWords)):\n",
    "        rawWords[i]=nltk.pos_tag(rawWords[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yH_xtHvftPdY"
   },
   "source": [
    "#### clean as in part (a) above, and extract the nouns only to obtain a bag-of-words tf-idf weighted vector representation using only the nouns\n",
    "\n",
    "> Ref: https://stackoverflow.com/questions/40167612/how-to-keep-only-the-noun-words-in-a-wordlist-python-nltk\n",
    "\n",
    "* Extracting Nouns from the raw data and storing them as array of arrays of documents\n",
    "* Cleaning\n",
    "   * Remove stop words\n",
    "   * Remove numbers and other non-letter characters\n",
    "   * Stem the words \n",
    "* Feature Extraction using TF-IDF  Vectorizer\n",
    "* Split Extracted Noun dataset into test-train\n",
    "* Train the classifiers NB, SVM and report Confusion Matrices for same\n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YUV72tcVtPdY"
   },
   "outputs": [],
   "source": [
    "#Extracting Nouns from the raw data\n",
    "\n",
    "ExtractedNouns = []*len(rawWords)\n",
    "\n",
    "for i in range(len(rawWords)):\n",
    "   ExtractedNouns.append([word for word,pos in rawWords[i] if (pos in ['NN','NNP','NNS','NNPS'])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "agWZN_7otPda"
   },
   "outputs": [],
   "source": [
    "#Converting all into lower case words before removing stop words\n",
    "for i in range(len(ExtractedNouns)):\n",
    "    for j in range(len(ExtractedNouns[i])):\n",
    "        ExtractedNouns[i][j] = ExtractedNouns[i][j].lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Rhkn-9IntPdd"
   },
   "outputs": [],
   "source": [
    "#Remove Stop words\n",
    "#Get all stop words from NLTK\n",
    "#Ref: https://www.geeksforgeeks.org/removing-stop-words-nltk-python/\n",
    "stop_words = set(stopwords.words('english')) \n",
    "\n",
    "for i in range(len(ExtractedNouns)):\n",
    "        ExtractedNouns[i] = [w for w in ExtractedNouns[i] if not w in stop_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9adZ2RWztPdh"
   },
   "outputs": [],
   "source": [
    "#Remove numbers and other non-letter characters\n",
    "for i in range(len(ExtractedNouns)):\n",
    "        ExtractedNouns[i] = [w for w in ExtractedNouns[i] if w.isalpha()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RL675aMhtPdo"
   },
   "outputs": [],
   "source": [
    "#Stem the words\n",
    "#Ref: Lab Text Processing\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "ps = PorterStemmer()\n",
    "\n",
    "for i in range(len(ExtractedNouns)):\n",
    "    for j in range(len(ExtractedNouns[i])):\n",
    "        ExtractedNouns[i][j] = ps.stem(ExtractedNouns[i][j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4eiXQODYtPds"
   },
   "outputs": [],
   "source": [
    "#Converting back to String\n",
    "#Ref: https://www.decalage.info/en/python/print_list\n",
    "cleanedNouns = [] \n",
    "\n",
    "for i in range(len(tokenizedWords)):\n",
    "        joinedString  = ' '.join(ExtractedNouns[i])\n",
    "        cleanedNouns.append(joinedString)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "QoofcC0htPdw",
    "outputId": "52121617-d75f-4a09-9f14-e038a8b326f5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2034, 15279)\n"
     ]
    }
   ],
   "source": [
    "#Using TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer_Nouns = TfidfVectorizer()\n",
    "X_Nouns = vectorizer_Nouns.fit_transform(cleanedNouns)\n",
    "#print(vectorizer.get_feature_names())\n",
    "print(X_Nouns.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "sqWBMhgBkfCB",
    "outputId": "6946ab96-f399-420e-b09d-000e86502fa9"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tfidf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>rych</th>\n",
       "      <td>0.503316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>file</th>\n",
       "      <td>0.430910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>textur</th>\n",
       "      <td>0.375352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hawk</th>\n",
       "      <td>0.263366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rule</th>\n",
       "      <td>0.255505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>format</th>\n",
       "      <td>0.176297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rychard</th>\n",
       "      <td>0.167772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>edinburgh</th>\n",
       "      <td>0.153098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>inform</th>\n",
       "      <td>0.142292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>psycholog</th>\n",
       "      <td>0.128147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>virtual</th>\n",
       "      <td>0.125117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>default</th>\n",
       "      <td>0.122468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>anyon</th>\n",
       "      <td>0.114062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>orient</th>\n",
       "      <td>0.111161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tel</th>\n",
       "      <td>0.107180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>univ</th>\n",
       "      <td>0.103320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>plane</th>\n",
       "      <td>0.102336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>map</th>\n",
       "      <td>0.100494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>environ</th>\n",
       "      <td>0.096487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model</th>\n",
       "      <td>0.089417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>laboratori</th>\n",
       "      <td>0.087904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dept</th>\n",
       "      <td>0.086952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fax</th>\n",
       "      <td>0.085820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hi</th>\n",
       "      <td>0.085820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>email</th>\n",
       "      <td>0.084537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>posit</th>\n",
       "      <td>0.082938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>noth</th>\n",
       "      <td>0.070927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>line</th>\n",
       "      <td>0.022269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>subject</th>\n",
       "      <td>0.022171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>petrich</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fwi</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fwhm</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fwd</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fw</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fullview</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fume</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fun</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>function</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fund</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fundament</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fundamentalist</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fundi</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fundrais</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fung</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>funk</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>funni</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>furnish</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>furrow</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>furthermor</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>furusawa</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fuse</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fuselag</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fusion</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fuss</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fussr</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>futher</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>futur</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fuzzi</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fv</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>l</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>15279 rows  1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   tfidf\n",
       "rych            0.503316\n",
       "file            0.430910\n",
       "textur          0.375352\n",
       "hawk            0.263366\n",
       "rule            0.255505\n",
       "format          0.176297\n",
       "rychard         0.167772\n",
       "edinburgh       0.153098\n",
       "inform          0.142292\n",
       "psycholog       0.128147\n",
       "virtual         0.125117\n",
       "default         0.122468\n",
       "anyon           0.114062\n",
       "orient          0.111161\n",
       "tel             0.107180\n",
       "univ            0.103320\n",
       "plane           0.102336\n",
       "map             0.100494\n",
       "environ         0.096487\n",
       "model           0.089417\n",
       "laboratori      0.087904\n",
       "dept            0.086952\n",
       "fax             0.085820\n",
       "hi              0.085820\n",
       "email           0.084537\n",
       "posit           0.082938\n",
       "noth            0.070927\n",
       "line            0.022269\n",
       "subject         0.022171\n",
       "petrich         0.000000\n",
       "...                  ...\n",
       "fwi             0.000000\n",
       "fwhm            0.000000\n",
       "fwd             0.000000\n",
       "fw              0.000000\n",
       "fullview        0.000000\n",
       "fume            0.000000\n",
       "fun             0.000000\n",
       "function        0.000000\n",
       "fund            0.000000\n",
       "fundament       0.000000\n",
       "fundamentalist  0.000000\n",
       "fundi           0.000000\n",
       "fundrais        0.000000\n",
       "fung            0.000000\n",
       "funk            0.000000\n",
       "funni           0.000000\n",
       "furnish         0.000000\n",
       "furrow          0.000000\n",
       "furthermor      0.000000\n",
       "furusawa        0.000000\n",
       "fuse            0.000000\n",
       "fuselag         0.000000\n",
       "fusion          0.000000\n",
       "fuss            0.000000\n",
       "fussr           0.000000\n",
       "futher          0.000000\n",
       "futur           0.000000\n",
       "fuzzi           0.000000\n",
       "fv              0.000000\n",
       "l              0.000000\n",
       "\n",
       "[15279 rows x 1 columns]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " #first document TF_IDF values\n",
    "first_vector_tfidfvectorizer=X_Nouns[0]\n",
    " \n",
    "# Print the values\n",
    "df = pd.DataFrame(first_vector_tfidfvectorizer.T.todense(), index=vectorizer_Nouns.get_feature_names(), columns=[\"tfidf\"])\n",
    "df.sort_values(by=[\"tfidf\"],ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uj52uHSItPd1"
   },
   "outputs": [],
   "source": [
    "#Split Noun data as Test and Train\n",
    "from sklearn.model_selection import train_test_split\n",
    "train_X_Nouns,test_X_Nouns,train_Y_Nouns,test_Y_Nouns=train_test_split(X_Nouns,newsgroups_train.target,test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4JrwS7UjtPd3"
   },
   "outputs": [],
   "source": [
    "# Train Multinomial NB and report confusion matrix.\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "clf = MultinomialNB().fit(train_X_Nouns, train_Y_Nouns)\n",
    "predictedNouns = clf.predict(test_X_Nouns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CF3zHGGxtPd7",
    "outputId": "eb2587e7-e196-47f7-d90b-97be571843d4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score : 91.65302782324059\n",
      "Confusion Matrix :\n",
      "[[135   0   1   0]\n",
      " [  0 177   4   0]\n",
      " [  0   4 184   0]\n",
      " [ 37   2   3  64]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy Score :\",accuracy_score(predictedNouns, test_Y_Nouns)*100)\n",
    "print(\"Confusion Matrix :\")\n",
    "print(confusion_matrix(test_Y_Nouns, predictedNouns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TtkDJXwWtPd9",
    "outputId": "fbafec5a-9cdb-40ba-c454-e81022798c3b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score : 94.92635024549918\n",
      "Confusion Matrix :\n",
      "[[126   0   1   9]\n",
      " [  0 181   0   0]\n",
      " [  0   8 178   2]\n",
      " [  9   2   0  95]]\n"
     ]
    }
   ],
   "source": [
    "#Train SVM and report confusion matrix.\n",
    "from sklearn import svm\n",
    "\n",
    "SVMNouns = svm.SVC(C=1.0, kernel='linear', degree=3, gamma='auto')\n",
    "SVMNouns.fit(train_X_Nouns, train_Y_Nouns)\n",
    "# predict the labels on validation dataset\n",
    "predictions_SVM_Nouns = SVMNouns.predict(test_X_Nouns)\n",
    "# Use accuracy_score function to get the accuracy\n",
    "print(\"Accuracy Score :\",accuracy_score(predictions_SVM_Nouns, test_Y_Nouns)*100)\n",
    "print(\"Confusion Matrix :\")\n",
    "print(confusion_matrix(test_Y_Nouns, predictions_SVM_Nouns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UPKKDtG3tPeC"
   },
   "source": [
    "####  How does this accuracy compare with that of part(c)?\n",
    "\n",
    "Ref: https://www.quora.com/Which-algorithm-is-better-for-implementing-to-get-high-accuracy-in-analysis-Naive-Bayes-or-a-support-vector-machine-SVM\n",
    "\n",
    "\n",
    "> There is not much difference in accuracy results of SVM by extracting nouns while compared to previous one .The slight decrease in accuracy compared to the previous one might be due to  slightly less number of large features, as only extracted nouns is taken into account to obtain bag of words. But there is huge difference in accuracy results for NB,this might be because by properly extracting bag of words in general NB performs pretty well for smaller datasets compared to SVM. SVM is more suitable for complex large datsets. Here we extracted only nouns (proper without noise) and calculated TF-IDF values, which increased slightly might be other reason for the increase in NB accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wxVa5djCtPeD"
   },
   "source": [
    "#### How does the size of the vocabulary compare with that of part (c)?\n",
    "\n",
    "As shown below the vocabulary got decreased after noun extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xcDzAgzEtPeE",
    "outputId": "d8844525-4bee-4b00-9d14-d41149ee942d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary after noun extraction\n",
      "15279\n"
     ]
    }
   ],
   "source": [
    "#Vocabulary after noun extraction \n",
    "print(\"Vocabulary after noun extraction\")\n",
    "print (len(vectorizer_Nouns.vocabulary_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tbS7rZXBtPeH",
    "outputId": "5a1378e7-66be-42df-aba1-85a2d835d752"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabuylary before noun extraction\n",
      "18399\n"
     ]
    }
   ],
   "source": [
    "#Vocabuylary before noun extraction\n",
    "print(\"Vocabuylary before noun extraction\")\n",
    "print (len(vectorizer.vocabulary_))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Assignment2.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
